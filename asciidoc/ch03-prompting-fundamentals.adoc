== Chapter 3: Prompting Fundamentals

Prompting is how you communicate intent to a Large Language Model (LLM). The difference between a good prompt and a bad one determines whether Claude Code generates working code on the first try or requires five iterations of refinement. This chapter teaches you four techniques that dramatically improve results: chain-of-thought prompting, constraint-based prompting, few-shot examples, and upfront questioning.

The underlying principle is entropy reduction. Every prompt you send creates a probability distribution over possible outputs. Vague prompts produce high entropy: many equally likely outputs, most of them wrong. Precise prompts produce low entropy: few possible outputs, most of them correct. Your job as a prompt author is to reduce entropy by providing constraints that eliminate invalid possibilities before generation begins.

This is not about being verbose. It is about being specific in ways that matter.

=== The Anatomy of an Effective Prompt

Every effective prompt has three components: context, instruction, and constraints.

*Context* tells the LLM what exists and what matters. It includes relevant files, existing patterns, and domain knowledge.

*Instruction* tells the LLM what to do. It specifies the action, the location, and the expected output.

*Constraints* tell the LLM what boundaries to respect. They narrow the solution space by eliminating invalid approaches.

Here is a weak prompt:

....
Add user validation to the API
....

This prompt has instruction but lacks context and constraints. The LLM must guess where validation goes, what patterns to follow, what errors to return, and how to test it. Each guess is a coin flip. The result is generic code that does not fit your project.

Here is the same request with all three components:

....
Add validation to the createUser endpoint in src/api/users.ts

Context:
- Validation patterns are in src/utils/validation.ts
- Use Zod for schema validation
- Return Result<T, ValidationError>, never throw

Constraints:
- Validate email format (RFC 5322)
- Validate password (min 8 chars, requires number)
- Include JSDoc comments
- Add tests in tests/api/users.test.ts

Success criteria:
- Invalid requests return 400 with error details
- Valid requests proceed to user creation
- All tests pass
....

This prompt eliminates thousands of possible implementations. The LLM knows exactly what technology to use, what patterns to follow, and how to verify success. The result: working code on the first try.

=== Chain-of-Thought Prompting

LLMs have a tendency to jump straight to implementation. You ask for a function, they generate code. The problem: they skip the reasoning that catches edge cases, error handling, and state management issues.

Chain-of-thought prompting forces the LLM to reason before implementing. Instead of asking for code directly, you ask for analysis first.

==== The Pattern

....
Before implementing [FEATURE], think through:

1. What are all the steps in this process?
2. What can go wrong at each step?
3. How should errors be handled?
4. What state transitions occur?
5. What needs to be logged?

After reasoning through this, implement the solution.
....

==== When Chain-of-Thought Matters

Use chain-of-thought for complex logic where correctness matters:

* *Multi-step workflows*: Payment processing, user onboarding, data pipelines
* *External API integrations*: Network failures, rate limits, timeouts
* *State machines*: Order status, subscription lifecycle, workflow stages
* *Security-critical code*: Authentication, authorization, data validation

Skip chain-of-thought for simple tasks:

* CRUD operations with no business logic
* Pure functions with obvious implementations
* Configuration files
* Simple utility functions

==== Example: Payment Processing

Without chain-of-thought, you might get this:

[source,typescript]
----
async function processPayment(amount: number, userId: string) {
  const charge = await stripe.charges.create({
    amount,
    currency: 'usd',
    customer: userId,
  });
  return { success: true, chargeId: charge.id };
}
----

This code works for the happy path and fails everywhere else. No input validation. No error handling. No retry logic. No idempotency. No logging.

With chain-of-thought, you get a different result. The prompt:

....
Before implementing the payment processing flow, think through:

1. What are all the steps in the payment flow?
2. What can go wrong at each step?
3. How should we handle each error?
4. What state transitions happen?
5. What needs to be logged/audited?

After reasoning through this, implement the solution.
....

The LLM now produces reasoning before code:

[source,markdown]
----
## Step 1: Payment Flow Steps

1. Validate input (amount, userId, payment method)
2. Check authorization (user owns the order)
3. Create idempotency key (prevent duplicates)
4. Create Stripe charge
5. Update order status to 'paid'
6. Send confirmation email
7. Log payment event

## Step 2: Failure Modes

- Invalid input: negative amount, missing userId
- Card declined: insufficient funds, expired card
- Network failure: timeout, connection error
- Database failure: can't update order status

## Step 3: Error Handling

- Input validation: Return immediately with validation errors
- Card declined: Return user-friendly message
- Network failures: Retry with exponential backoff (max 3 attempts)
- Database failures: Critical, alert on-call, rollback charge
----

The implementation that follows handles all these cases. The code is 10x longer but actually works in production.

The mental model: chain-of-thought converts runtime bugs into compile-time requirements. Instead of discovering edge cases through crashes, you discover them during planning.

=== Constraint-Based Prompting

Constraints are rules that eliminate invalid outputs. Every constraint you add reduces entropy by narrowing the valid solution space.

==== Declarative Over Imperative

Compare these two approaches:

*Imperative* (tells HOW):

....
First, read schema.ts to understand the User type.
Then, find the User type definition.
Then, add an email field with type string.
Then, add validation for email format using regex.
Then, update the insert function to include email.
....

*Declarative* (tells WHAT):

....
User type MUST have email: string field.
Email MUST be validated with RFC 5322 format.
All type changes MUST have corresponding Zod schema updates.
All type changes MUST have test coverage.
....

Imperative prompts are fragile. If any step fails or the codebase structure differs from expectations, the entire workflow breaks. Declarative constraints adapt. The LLM finds its own path to satisfy the constraints regardless of how the codebase is organized.

==== Types of Constraints

*Format constraints* specify output structure:

....
All functions MUST have JSDoc comments.
All async functions MUST return Result<T, E>, never throw.
All API responses MUST follow { success, data?, error? } shape.
....

*Behavior constraints* specify what the code must do:

....
All user inputs MUST be validated before processing.
All database operations MUST be wrapped in transactions.
All external API calls MUST have timeout and retry logic.
....

*Scope constraints* specify boundaries:

....
Do NOT modify files outside src/payments/.
Do NOT change existing public interfaces.
Do NOT add new dependencies without explicit approval.
....

*Performance constraints* specify non-functional requirements:

....
Response time MUST be under 200ms for 95th percentile.
Memory usage MUST not exceed 512MB.
Batch processing MUST handle 10,000 records per minute.
....

==== The Constraint Funnel

Think of constraints as a funnel. Each constraint eliminates possible outputs:

....
All syntactically valid programs         [1,000,000 possibilities]
    ↓ Type constraints
Type-safe programs                       [10,000 possibilities]
    ↓ Format constraints
Consistently formatted programs          [1,000 possibilities]
    ↓ Behavior constraints (tests)
Correct programs                         [100 possibilities]
    ↓ Style constraints
Programs matching your codebase          [10 possibilities]
....

Each layer reduces entropy by an order of magnitude. The result: predictable, correct, maintainable code.

=== Few-Shot Prompting with Project Examples

Abstract descriptions of patterns are ambiguous. Concrete examples are not.

When you tell an LLM "`use dependency injection with factory functions,`" it has many valid interpretations. When you show it two examples from your codebase that use dependency injection with factory functions, it has exactly one interpretation: match those examples.

==== How Many Examples?

Research and practice converge on 2-3 examples as the optimal number.

*0 examples* (zero-shot): 40-60% accuracy. The LLM guesses based on general knowledge.

*1 example* (one-shot): 60-75% accuracy. The LLM might treat it as a special case instead of a pattern.

*2-3 examples* (few-shot): 85-95% accuracy. The LLM identifies what varies versus what stays consistent across examples.

*4{plus} examples*: Diminishing returns. More tokens consumed, marginal accuracy improvement.

==== Example Selection Criteria

Choose examples that:

[arabic]
. *Demonstrate the pattern clearly*: Pick typical cases, not edge cases
. *Show consistency*: All examples follow the same structure
. *Cover typical complexity*: Include at least one example with real business logic
. *Are current*: Reflect your latest conventions, not legacy code

==== The Few-Shot Template

[source,markdown]
----
# Pattern: Service Layer

Here are examples of how we implement services in this codebase:

## Example 1: User Service

**File**: packages/domain/src/users/user-service.ts

[paste full code]

## Example 2: Campaign Service

**File**: packages/domain/src/campaigns/campaign-service.ts

[paste full code]

---

# Your Task

Now create a Post Service following the same pattern:
- Factory function:
  createPostService(deps: PostServiceDeps): PostService
- Methods: createPost, publishPost, deletePost
- Return Result<T, E> type (never throw)
- Include JSDoc comments
----

The LLM reads both examples, identifies the common structure, and generates a Post Service that matches exactly. First-try accuracy jumps from 50% to 90%.

==== Where to Store Examples

Put few-shot examples in domain-specific CLAUDE.md files:

....
packages/
  domain/
    CLAUDE.md  ← service and repository patterns
  api/
    CLAUDE.md  ← API route patterns
  tests/
    CLAUDE.md  ← test file patterns
....

Claude Code reads these automatically. You do not need to paste examples into every prompt.

=== Upfront Questioning

When requirements are vague, asking questions before implementing reduces iteration cycles dramatically.

==== The Search Space Problem

Consider this request: "`Add a settings page.`"

Possible interpretations multiply: - Modal vs full page vs sidebar - Single form vs tabbed sections - Auto-save vs explicit save button - Dark mode support or not - Mobile responsive or desktop only

Each unclarified dimension doubles the possibilities. Five dimensions means 32 different implementations, and you probably only want one.

==== The Pattern

Tell Claude Code explicitly:

....
Before implementing, ask me questions to clarify requirements.
....

The agent then interviews you about edge cases, preferences, and constraints. Only after gathering answers does it propose implementation.

==== Example Exchange

*User*: Add a notifications system to the app.

*Claude Code*: Before implementing, I have some questions: 1. What types of notifications? (system alerts, user actions, reminders) 2. In-app only, or email/push as well? 3. Should users configure notification preferences? 4. Real-time (WebSocket) or polling? 5. How long should notifications persist? 6. Any grouping/batching of similar notifications? 7. What is the read/unread UX?

*User*: Just in-app, real-time, user actions only, persist 7 days, no grouping, simple dot indicator for unread.

*Claude Code*: Now implementing with those constraints…

The search space collapsed from hundreds of combinations to one clear path. Implementation time drops. Iteration count drops to one.

==== When to Use Upfront Questioning

[width="100%",cols="60%,40%",options="header"]
|===
|Scenario |Use Upfront Questioning?
|Vague feature request |Yes
|User Interface/User Experience (UI/UX) work |Yes
|New domain you are unfamiliar with |Yes
|Clear, well-specified task |No
|Bug fix with reproduction steps |No
|Refactoring with defined scope |No
|===

The cost of asking questions is linear. The cost of exploring wrong solutions is exponential. Front-load the questions.

=== Combining Techniques

These four techniques work together. The most effective prompts combine multiple approaches.

*Chain-of-thought {plus} Constraints*: Ask the LLM to reason through requirements, then implement with explicit constraints based on that reasoning.

....
Think through what validation the payment endpoint needs.
Then implement with these constraints:
- Use Zod schemas
- Return Result<T, PaymentError>
- Log all validation failures
....

*Few-shot {plus} Upfront Questioning*: Show examples of similar features, then ask clarifying questions about the differences.

....
Here are examples of how we implement notification handlers:
[examples]

Before implementing the new alert handler, what questions do you have about:
- Alert severity levels
- Notification channels
- Retry behavior
....

*Constraints {plus} Few-shot*: Combine explicit rules with concrete examples. The constraints define what must be true. The examples show how to achieve it.

The combination reduces entropy more than any single technique. Constraints eliminate invalid approaches. Examples show the valid approach. Chain-of-thought ensures edge cases are considered. Upfront questioning fills gaps before implementation begins.

Start with one technique. Add others as needed. Complex features benefit from all four. Simple tasks need only constraints.

=== Anti-Patterns: What NOT to Do

These patterns produce poor results. Avoid them.

==== Vague Prompts

....
Make it better.
Add authentication.
Fix the bugs.
....

These prompts have maximum entropy. Every possible implementation is equally likely. The result is generic code that does not fit your project.

==== Over-Constrained Prompts

....
Use exactly bcrypt with salt rounds 10 for password hashing.
Loop through users with a for-i loop.
Store results in variable called 'finalResult'.
....

Over-specification prevents the LLM from choosing better approaches. Specify WHAT must be true, not HOW to achieve it.

==== Missing Context

....
Add validation to the handler.
....

Which handler? What validation rules? What patterns exist? Without context, the LLM cannot produce code that fits your codebase.

==== The "`Just Do It`" Trap

Skipping exploration to save time costs more time in iteration. Five minutes of questions saves thirty minutes of revision.

==== Mixing Exploration and Implementation

Bad:

....
How does authentication work? Also implement a new login endpoint.
....

Good:

....
Step 1: How does authentication work in this codebase? Show me examples.
Step 2 (after understanding):
Implement the login endpoint following these patterns.
....

Separate the modes. Explore first, then implement with informed context.

=== Exercises

==== Exercise 1: Build a Prompting Toolkit

Create a personal prompting toolkit with: - 3 chain-of-thought templates for common tasks in your domain - A constraint checklist for code generation - 5 few-shot examples from your codebase - A list of upfront questions for new features

Test each component on a real task. Document what works.

==== Exercise 2: Prompt Comparison Experiment

Take the same task and try it with: 1. A vague prompt ("`add user authentication`") 2. A constrained prompt (with specific requirements) 3. A few-shot prompt (with examples)

Compare the results. Document iteration counts, code quality, and pattern adherence.

==== Exercise 3: The Clarification Challenge

Practice upfront questioning: 1. Take a feature request from your backlog 2. Write 5-7 clarifying questions 3. Ask Claude to answer them based on your codebase 4. Implement only after answers are complete

Measure: How many iterations did this save compared to implementing immediately?

=== Summary

Effective prompting reduces entropy by providing context, instructions, and constraints that eliminate invalid outputs before generation begins.

Four techniques produce consistently better results:

*Chain-of-thought prompting* forces reasoning before implementation. Use it for complex logic where edge cases, error handling, and state management matter. Skip it for simple tasks.

*Constraint-based prompting* specifies WHAT must be true, not HOW to achieve it. Declarative constraints adapt to codebase structure. Imperative instructions break when assumptions fail.

*Few-shot prompting* teaches patterns through concrete examples. Two to three examples from your codebase produce 85-95% pattern accuracy. Store examples in CLAUDE.md files.

*Upfront questioning* collapses the search space before implementation. The cost of questions is linear. The cost of wrong implementations is exponential.

The underlying principle: every constraint you add eliminates possible outputs. Enough constraints and only correct outputs remain.

The next chapter applies these prompting fundamentals to CLAUDE.md files. You will learn to structure project context that makes every prompt more effective without additional effort. See <<_chapter_4_writing_your_first_claude_md,Chapter 4: Writing Your First CLAUDE.md>>.

'''''

NOTE: *Companion Code*: All 3 code examples for this chapter are available at https://github.com/Just-Understanding-Data-Ltd/compound-engineering-book/tree/main/examples/ch03[examples/ch03/]

_Related chapters:_ - <<_chapter_2_getting_started_with_claude_code,Chapter 2: Getting Started with Claude Code>> for tool usage and basic patterns - <<_chapter_4_writing_your_first_claude_md,Chapter 4: Writing Your First CLAUDE.md>> for project context configuration - <<_chapter_9_context_engineering_deep_dive,Chapter 9: Context Engineering Deep Dive>> for advanced information theory applications
