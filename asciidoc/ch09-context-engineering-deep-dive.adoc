== Chapter 9: Context Engineering Deep Dive

Context windows are the most constrained resource in AI-assisted development. You have 200,000 tokens at most, and everything the model knows about your task must fit within that space. This chapter treats context windows as information channels governed by mathematical principles. By understanding how information flows through these channels, you can predictably control model behavior, scale agent capabilities beyond context limits, and debug generation failures systematically.

=== Information Theory Foundations

Claude Shannon founded information theory in 1948 to study communication systems. His insights about channels, noise, and uncertainty apply directly to working with Large Language Models (LLMs). When you provide context to Claude, you’re transmitting information through a channel with hard capacity limits.

==== Entropy: Measuring Uncertainty

Entropy measures uncertainty in a probability distribution. The formula is simple:

....
H(X) = -∑ P(x) log₂ P(x)
....

For code generation, entropy measures how many valid programs could satisfy your request. High entropy means many equally likely outputs. Low entropy means the model has converged on a few specific implementations.

Consider this vague prompt:

[source,typescript]
----
// Prompt: "Write a function to process data"

// Possible outputs (all equally likely):
function processData(data: any): any { ... }           // P = 0.2
function processData(data: any): void { ... }          // P = 0.2
function processData(data: any): boolean { ... }       // P = 0.2
function processData(data: any[]): string[] { ... }    // P = 0.2
function processData(data: unknown): never { ... }     // P = 0.2

// Entropy: H ≈ 2.32 bits (high uncertainty)
----

Now add constraints:

[source,typescript]
----
// Prompt + types + tests + context:
function processData(
  data: Array<{ id: number; value: string }>
): ProcessResult {
  // Only 1-2 valid implementations that satisfy all constraints
  // Entropy: H ≈ 0.5 bits (low uncertainty)
}
----

Quality gates work by filtering the state space:

....
All syntactically valid programs:  H = 20 bits  (1M+ programs)
                ↓
         [Type Checker]
                ↓
Type-safe programs:                H = 15 bits  (32K programs)
                ↓
           [Linter]
                ↓
Type-safe, clean programs:         H = 12 bits  (4K programs)
                ↓
           [Tests]
                ↓
Type-safe, clean, correct programs: H = 5 bits  (32 programs)
....

Each gate reduces entropy multiplicatively. Going from H=20 to H=5 eliminates 99.997% of possible programs.

You can estimate entropy through test failure rates. High failure rates (30-50%) indicate high entropy and unpredictable behavior. Low rates (under 5%) indicate the model has converged on correct implementations.

==== Information Content: Why Types Beat Comments

Information content measures how much you learn when a constraint is satisfied:

....
I(x) = -log₂ P(x)
....

Different constraints provide vastly different amounts of information:

*Types* (high information):

[source,typescript]
----
// This constraint eliminates ~90% of possible implementations
function processUser(user: User): Promise<Result>

// Information provided: ~3.3 bits
----

*Tests* (very high information):

[source,typescript]
----
// This test eliminates ~95% of type-safe implementations
test('processUser returns success=true for valid user', () => {
  expect(result.success).toBe(true);
});

// Information provided: ~4.3 bits
----

*Comments* (low information):

[source,typescript]
----
// This comment eliminates ~10% of implementations
// Process the user data

// Information provided: ~0.15 bits
----

Types provide 11 times more information per token than comments. This explains why showing the model type signatures and test cases produces better results than verbose documentation. When filling your context window, prioritize high-information content: type definitions, test cases, and working code examples.

==== Mutual Information: Measuring Context Effectiveness

Mutual information (MI) captures how much knowing your context tells you about the output:

....
I(X;Y) = H(X) - H(X|Y)
....

High mutual information means your context strongly determines the output. Low mutual information means the context isn’t helping.

You can measure mutual information through output variance. Generate the same prompt 10 times: - 1-2 unique outputs: High mutual information (good context) - 3-5 unique outputs: Medium mutual information (improve context) - 6{plus} unique outputs: Low mutual information (context needs work)

High-MI patterns include: - Working examples over explanations - Concrete constraints over vague guidelines - Anti-patterns showing what NOT to do - Multiple examples rather than a single example

Compare low-MI context:

[source,markdown]
----
# Context
Write clean, maintainable code.
Use good patterns.
Handle errors properly.
----

With high-MI context:

[source,markdown]
----
# Context

✅ DO THIS:
function authenticate(email: string, password: string): AuthResult {
  if (!email || !password) {
    return { success: false, error: 'Email and password required' };
  }
  // ... implementation
}

❌ DON'T DO THIS:
function authenticate(email: string, password: string): User {
  if (!email) throw new Error('Email required');  // Don't throw
  // ... implementation
}
----

The second context has much higher mutual information. It strongly constrains what the output should look like.

==== Channel Capacity: Working Within Limits

Channel capacity is the maximum information that can reliably pass through a context window:

[source,python]
----
# Claude context window
max_tokens = 200_000

# If each token encodes ~4 bits of information
channel_capacity = 200_000 * 4 = 800_000 bits = 100 KB
----

You cannot exceed this capacity. If you try, either early context gets truncated, information density decreases, or data gets compressed lossy through summarization.

Optimization strategies: 1. *Maximize information density*: Use types and tests over verbose documentation 2. *Hierarchical context loading*: Load only relevant context for the current task 3. *Prompt caching*: Cache stable, high-information content

Target metrics: - Utilization: 60-80% of capacity (stay below limits) - Information density: 3.5{plus} bits/token - Output variance: Under 2 unique outputs for the same prompt

=== Progressive Disclosure Patterns

Progressive disclosure organizes context in layers that load on-demand. Instead of cramming everything into the system prompt, you provide minimal metadata upfront and expand to full instructions only when needed.

==== Three-Level Architecture

*Level 1: Metadata Layer* (always loaded)

[source,yaml]
----
# SKILL.md frontmatter
---
name: pdf-manipulation
description: Extract text, fill forms, merge/split PDFs
triggers:
  - "pdf"
  - "form"
  - "document"
---
----

Cost: ~50-100 tokens per skill. You can have dozens of skills for under 2,000 tokens.

*Level 2: Core Instructions* (loaded when relevant)

[source,markdown]
----
# PDF Manipulation Skill

## Capabilities
- Extract text from PDFs using `pdf-extract` tool
- Fill form fields using `pdf-form` tool
- Merge multiple PDFs with `pdf-merge`

## Usage Patterns
### Text Extraction
1. Identify the PDF file path
2. Call `pdf-extract --input <path> --output <format>`
3. Process the extracted text

See `forms.md` for detailed form-filling instructions.
----

Cost: ~500-2000 tokens per skill, loaded only when triggered.

*Level 3: Supplementary Resources* (loaded as needed)

Deep-dive information for edge cases, loaded only when explicitly referenced.

==== Implementation Structure

....
skills/
├── pdf/
│   ├── SKILL.md          # Level 1 + 2
│   ├── forms.md          # Level 3
│   └── reference.md      # Level 3
├── git/
│   ├── SKILL.md
│   └── workflows.md
└── testing/
    ├── SKILL.md
    └── fixtures.md
....

The agent starts with metadata from all skills. When it recognizes a PDF task, it loads the full PDF skill. When it encounters form-filling, it loads the supplementary forms reference.

==== Benefits

*Scalability*: Add unlimited skills without context explosion

....
10 skills × 50 tokens metadata = 500 tokens always loaded
vs.
10 skills × 1500 tokens full = 15,000 tokens (30x more)
....

*Cost efficiency*: 87% savings by loading only relevant context

*Unbounded capability*: Agents with filesystem access can read files on-demand, extending capability beyond the context window entirely.

=== Context Rot and Auto-Compacting

Long sessions accumulate stale information. This is context rot.

==== Symptoms

You know you have context rot when the AI: - References code you deleted 50 messages ago - Suggests old architecture patterns you abandoned - Confuses current state with historical state - Hallucinates about files that never existed - Decreases accuracy as the conversation grows

==== Signal-to-Noise Degradation

....
Messages 1-20:   90% signal (mostly relevant)
Messages 21-50:  60% signal (mix of current and obsolete)
Messages 51-100: 30% signal (stale context dominant)
Messages 100+:   10% signal (buried in history)
....

==== Claude Code’s Auto-Compacting

Claude Code monitors context size and automatically triggers compacting when context grows large. It summarizes completed work, preserves key decisions and current state, and removes intermediate debugging steps.

....
BEFORE compacting:
- 150 messages
- 100K tokens
- References to deleted code
- 60% generation accuracy

AFTER compacting:
- 10 messages
- 15K tokens
- Clear current state
- 95% generation accuracy
....

==== Manual Compacting Strategies

*Task-driven compacting*: Summarize every 5-10 completed tasks

*Recursive compacting* (multi-level): - Level 1: 1-2 sentences per completed task - Level 2: Paragraph per completed feature - Level 3: DIGEST.md per milestone - Level 4: Archive to CHANGELOG.md

*Compacting prompt pattern*:

....
"Summarize all completed work:
1. What features were implemented?
2. What architectural decisions were made?
3. What's the current state?
4. What's still pending?

Output: Compact summary (max 500 words)"
....

==== When to Compact

* After completing 5-10 tasks
* After finishing a feature
* When switching contexts to a different package
* When AI references deleted or outdated code
* After 100{plus} messages
* Before starting a new major feature

=== Context-Efficient Backpressure

When tests pass, developers waste context conveying results that need fewer than 10 tokens to communicate. Claude models perform optimally within approximately 75K tokens. Beyond this, agents miss obvious errors and ignore instructions.

==== The run++_++silent Pattern

Swallow output on success, dump on failure:

[source,bash]
----
run_silent() {
    local description="$1"
    local command="$2"
    local tmp_file=$(mktemp)

    if eval "$command" > "$tmp_file" 2>&1; then
        printf "  ✓ %s\n" "$description"
        rm -f "$tmp_file"
        return 0
    else
        local exit_code=$?
        printf "  ✗ %s\n" "$description"
        cat "$tmp_file"
        rm -f "$tmp_file"
        return $exit_code
    fi
}

# Usage
run_silent "Auth tests" "pytest tests/auth/"
run_silent "Utils tests" "pytest tests/utils/"
run_silent "API tests" "pytest tests/api/"
----

*Output on success:*

....
✓ Auth tests
✓ Utils tests
✓ API tests
....

*Output on failure:*

....
✓ Auth tests
✓ Utils tests
✗ API tests

FAIL src/api/users.test.ts
● should validate email format
  Expected: true
  Received: false
....

==== Progressive Refinement

Enable failFast to process one failure at a time:

[source,bash]
----
pytest -x tests/           # Python
jest --bail                 # JavaScript
go test -failfast ./...     # Go
----

==== Key Principle

If you already know what matters, don’t leave it to a model to churn through thousands of junk tokens to decide. Deterministic output beats non-deterministic parsing.

=== Systematic Context Debugging Framework

When AI doesn’t produce desired output, follow a hierarchical debugging protocol ordered by likelihood of success.

==== The Four-Layer Hierarchy

....
Layer 1: CONTEXT (60% of issues)
Add missing information, files, examples, architecture
                         ↓
Layer 2: PROMPTING (25% of issues)
Refine instructions, add examples, clarify constraints
                         ↓
Layer 3: MODEL POWER (10% of issues)
Escalate to more powerful model for complex reasoning
                         ↓
Layer 4: MANUAL OVERRIDE (5% of issues)
Recognize when human intuition is needed
....

==== Layer 1: Context (60% of Issues)

*Problem signature*: AI produces plausible but incorrect code that doesn’t fit the codebase.

*Debugging checklist*: 1. Include relevant code files showing existing patterns 2. Provide system architecture and design decisions 3. Include error messages and stack traces 4. Show database schemas and API contracts 5. Provide examples of expected behavior

*Example before*:

[source,typescript]
----
// Prompt: "Create a user authentication endpoint"
// AI generates generic code that doesn't match project patterns
----

*Example after*:

[source,typescript]
----
// Added context: Include existing auth endpoint as example
// @src/api/auth/register.ts

// Prompt: "Create a user authentication endpoint
// following the pattern in register.ts"

// AI generates code matching project conventions
----

==== Layer 2: Prompting (25% of Issues)

*Problem signature*: AI has context but output doesn’t meet requirements.

*Debugging checklist*: 1. Add specific examples of desired output 2. Include edge cases and constraints 3. Provide clear success criteria 4. Break complex tasks into steps 5. Use structured formats

*Example before*:

....
Prompt: "Format user data for display"
AI generates: Generic JSON formatting
....

*Example after*:

....
Prompt: "Format user data for display

Expected output:
Input: { id: 1, email: 'test@example.com', createdAt: '2025-01-15T10:30:00Z' }
Output: 'test@example.com (joined Jan 15, 2025)'"

AI generates: Exact format specified
....

==== Layer 3: Model Power (10% of Issues)

Only escalate when Layers 1 and 2 are exhausted. Some tasks genuinely need stronger reasoning. A real-time collaborative editing system with conflict resolution may require Claude Opus (Anthropic’s most capable model) where Claude Sonnet (the balanced cost-performance model) fails.

*Cost consideration*:

....
Claude Sonnet: $3 per 1M input tokens
Claude Opus: $15 per 1M input tokens (5x more expensive)

Strategy:
1. Try Sonnet with good context (90% success, low cost)
2. Escalate to Opus only for remaining 10% (high cost, rare)
....

==== Layer 4: Manual Override (5% of Issues)

*When to go manual*: - Deep domain expertise required (medical diagnosis, legal compliance) - Human intuition or creativity needed (brand identity, UX decisions) - Ambiguous or contradictory requirements - Legacy systems with tribal knowledge and no documentation

*Hybrid approach*: Human solves core problem, documents solution clearly, AI implements and scales the solution.

==== Time Comparison

....
Scenario: Missing context issue (60% of cases)

Unsystematic approach:
1. Try different model: 15 min ✗
2. Rewrite prompt: 10 min ✗
3. Try another tool: 20 min ✗
4. Finally add context: 5 min ✓
Total: 50 minutes

Systematic approach:
1. Add context first: 5 min ✓
Total: 5 minutes
....

=== Bringing It Together

Information theory explains why patterns work: - Context fills the channel up to capacity - Generation produces output with entropy proportional to constraint quality - Quality gates filter by eliminating invalid states - Final output has low entropy when constraints compound

For long sessions: - Progressive disclosure scales capabilities beyond context limits - Auto-compacting maintains freshness in 100{plus} message sessions - Backpressure keeps signal-to-noise high for accuracy

For cost optimization: - Prioritize high-density content: types and tests over documentation - Budget tokens by information density - Cache stable, high-MI content - Use Sonnet with good context before Opus

=== Exercises

==== Exercise 1: Measure Entropy in Your Codebase

[arabic]
. Pick a function you want to generate (user validation, API endpoint)
. Generate it 10 times with the same prompt, no context
. Count unique implementations produced
. Estimate entropy: H ≈ log₂(unique++_++outputs)
. Add one constraint (type signature)
. Repeat 10 times, count unique outputs
. Measure entropy reduction
. Add another constraint (test cases)
. Repeat and measure again
. Document how entropy dropped with each constraint

*Expected outcome*: You should see exponential entropy reduction as constraints stack.

==== Exercise 2: Design a Progressive Disclosure Skill

[arabic]
. Choose a domain (PDF processing, git operations, testing)
. Create Level 1 metadata (~50 tokens): name, description, triggers
. Write Level 2 core instructions (~1000 tokens): capabilities, common patterns
. Create Level 3 supplementary docs: advanced patterns, edge cases
. Simulate what the agent loads at each stage
. Compare token cost to flat loading (everything at once)

*Expected outcome*: 70%{plus} reduction in average tokens loaded per task.

==== Exercise 3: Debug Using the Hierarchical Protocol

[arabic]
. Find a real case where AI generated incorrect code
. Layer 1: Add relevant files, architecture, error logs. Fixed? Note time and stop.
. Layer 2: Add specific examples, edge cases, success criteria. Fixed? Note time and stop.
. Layer 3: Escalate to more powerful model. Fixed? Note time and stop.
. Layer 4: Manually implement core logic, have AI scale it.
. Analyze: Which layer fixed the issue? Time at each layer?

*Expected outcome*: Most issues resolve at Layer 1 (context), with faster resolution than trial-and-error.

=== Summary

Context engineering is the discipline of managing finite information capacity. You’ve learned:

* *Entropy* measures uncertainty. Quality gates reduce entropy exponentially by filtering invalid states.
* *Information content* explains why types provide 11x more value per token than comments.
* *Mutual information* captures context effectiveness. High MI means context strongly determines output.
* *Channel capacity* is the hard limit. Maximize information density, not just token count.
* *Progressive disclosure* scales capabilities beyond context limits by loading information on-demand.
* *Auto-compacting* prevents context rot in long sessions by summarizing completed work.
* *Backpressure* keeps the model in the optimal performance zone by suppressing noise from passing tests.
* *Hierarchical debugging* resolves 60% of issues at the context layer, saving time and cost.

The goal is code generation that is predictable, correct, and efficient. Not by luck, but by mathematical design.

'''''

____
*Companion Code*: All 7 code examples for this chapter are available at https://github.com/Just-Understanding-Data-Ltd/compound-engineering-book/tree/main/examples/ch09[examples/ch09/]
____

_Related chapters:_ - link:ch03-prompting-fundamentals.md[Chapter 3: Prompting Fundamentals] for foundational techniques that maximize information density - link:ch07-quality-gates-that-compound.md[Chapter 7: Quality Gates That Compound] for how verification stages reduce entropy exponentially - link:ch08-error-handling-and-debugging.md[Chapter 8: Error Handling & Debugging] for the Five-Point Diagnostic protocol - link:ch10-the-ralph-loop.md[Chapter 10: The RALPH Loop] for managing context in long-running agent sessions
