# PRD: Chapter 1 - "The Compound Systems Engineer"

**Status:** Draft
**Audience:** Software engineers (mid-career to senior) interested in leverage and systems thinking
**Word Count Target:** 5,500-7,000 words

---

## 1. Overview

Chapter 1 introduces the foundational philosophy of compound engineering by establishing a new career archetype: the Compound Systems Engineer. Unlike indie hackers (single-bet optimists), lifestyle founders (income-focused), or career engineers (resume-builders), the compound systems engineer plays a portfolio game where leverage compounds through reusable infrastructure, cognitive capital, and long-term systems thinking. This chapter answers the core question: "What game am I actually playing?" and explains why persistence is rational when leverage is positive, even if short-term outcomes are invisible. Readers will discover that the shift from typing code to orchestrating self-improving systems is not mystical—it's a learnable architecture discipline with measurable economics.

---

## 2. Learning Objectives

By the end of this chapter, readers will:

1. **Understand the Compound Systems Engineer archetype** - Differentiate this career path from indie hacking, lifestyle business, and traditional employment; recognize it as a deliberate, long-term strategy rather than a personality trait
2. **Recognize the three levels of engineering** - Internalize that Level 3 (systems that build systems) is where leverage lives; see their current work within this framework
3. **Embrace meta-engineering identity** - Shift from "I write code" to "I design systems that make future work cheaper"; understand this is a cognitive reframing, not a promotion
4. **Apply portfolio game economics** - Learn why treating projects as single bets guarantees failure; understand how to evaluate persistence using slope, not intercept
5. **Connect systems thinking to personal leverage** - See the direct causal link between observability, constraints, and compounding returns; identify where leverage is currently accumulating (or stalling) in their own work

---

## 3. Source Articles

- `@~/Desktop/knowledge-base/01-Compound-Engineering/my-doctrine.md` - The foundational philosophy, risk framework, and decision logic
- `@~/Desktop/knowledge-base/01-Compound-Engineering/context-engineering/the-meta-engineer-identity.md` - The identity shift and skill stack required
- `@~/Desktop/knowledge-base/01-Compound-Engineering/software-archetypes.md` - Archetype comparison and failure modes
- `@~/Desktop/knowledge-base/01-Compound-Engineering/index.md` - Quick reference and key metrics

---

## 4. Detailed Outline

### Section 1: The Problem (Why This Matters)
**Approximate length:** 800 words

#### 1.1 Binary Advice Collapses
- Most career advice reduces to false binaries: job vs. startup, employee vs. founder, ship fast vs. overthink
- These binaries work in low-variance domains (e.g., administrative jobs, linear startups)
- They fail catastrophically in high-variance, high-leverage domains (complex systems, AI infrastructure, solo engineering at scale)
- **Transition sentence:** "You need a different model if you're operating in asymmetric domains."

#### 1.2 The Single-Bet Trap
- Most people treat independent work (indie hacking, solo products) as a single sample from a distribution
- Implicit assumption: one product, one year, one outcome, immediate validation
- Reality: high-variance systems need many samples to produce reliable signal
- Show the failure pattern: Sample once → No validation → Abandon the whole category
- **Example:** "I tried indie hacking for a year. It didn't work." (often means they sampled once from a distribution)

#### 1.3 The Comparison Trap
- When peers stop and you continue, the implicit question becomes: "Who is right?"
- This is the wrong comparison
- The correct comparison: Are we playing the same game? Same leverage? Same runway? Same internal signals?
- Stopping can be rational for them AND continuing can be rational for you
- **Key insight:** "The game you're playing determines which moves are rational."

#### 1.4 What This Chapter Offers
- A mental model: the Compound Systems Engineer archetype
- A decision framework: when to persist, when to stop
- A reframing: identity as capability, not outcome
- Permission to operate differently than peers

---

### Section 2: The Compound Systems Engineer Archetype
**Approximate length:** 1,200 words

#### 2.1 What Is Compound Engineering?
- Three concise statements:
  - "Systems outlast products."
  - "Cognition outlives code."
  - "Leverage beats speed."
- Core definition: "An engineer who optimises for long-term leverage by building reusable infrastructure, treating code and cognition as capital, and playing a portfolio game rather than single bets."
- This is not indie hacking. This is not lifestyle business. This is not career advancement.
- **It is a distinct archetype with its own logic.**

#### 2.2 The Three Levels of Engineering
- **Level 1: Write code** - Most engineers stop here
  - Output: features, bug fixes, technical debt reduction
  - Time-to-competence: months
  - Leverage: linear (more code ≈ more time invested)
- **Level 2: Write systems** - Some engineers reach here
  - Output: architecture, frameworks, observability
  - Time-to-competence: years
  - Leverage: sublinear → linear (infrastructure is reusable, but still requires tuning per project)
- **Level 3: Write systems that write systems** - Meta-engineers operate here
  - Output: AI-assisted pipelines, self-improving harnesses, constraint-enforcing environments
  - Time-to-competence: 3-5 years of deliberate practice
  - Leverage: compound (future projects automatically inherit past investments)
- Diagram: Show the three levels as a pyramid with effort/time invested on Y-axis and leverage on X-axis
- **Key realization:** "You've been Level 1. You can reach Level 2 in years. Level 3 is where the game changes."

#### 2.3 The Meta-Engineer Identity
- The shift from builder to meta-builder:
  - From: "I write CRUD endpoints"
  - To: "I design API generation systems"
  - From: "I debug issues"
  - To: "I build observability that surfaces issues"
  - From: "I follow patterns"
  - To: "I create patterns"
- This is not a job title. This is a cognitive orientation.
- The skill stack meta-engineers develop:
  1. Mathematical reasoning (invariants, complexity, optimization)
  2. Systems thinking (feedback loops, constraints, emergent behavior)
  3. Architectural design (boundaries, contracts, DDD)
  4. Agent orchestration (automation, verification)
  5. Observability engineering (OTEL, metrics, traces)
  6. Infrastructure as code (Terraform, Docker, K8s)
  7. Core programming (TypeScript, Python, SQL)
- Most engineers develop only the bottom layers. Meta-engineers develop the full stack.
- **Permission statement:** "You don't need to be a genius to reach this. You need intentional practice and a 3-5 year horizon."

#### 2.4 The Compound Systems Engineer in Action
- Real-world example: AI rank tracker system
  - Level 1: "I built a monitoring dashboard"
  - Level 2: "I built a monitoring system with self-healing alerts"
  - Level 3: "I built a system where alerts self-fix AND agents can add new alert types without my code"
- Example output from each level:
  - Level 1: 300 lines of dashboard code
  - Level 2: 2000 lines of system code
  - Level 3: 500 lines of constraint definitions + agents that generate the implementation
- **Economics:** Level 3 scales differently because future features inherit the harness

---

### Section 3: The Game You're Playing
**Approximate length:** 1,200 words

#### 3.1 The Portfolio Game vs. The Single-Bet Game
- Single-bet game: One product, one outcome determines success/failure
  - Variance: extremely high
  - Expected value depends entirely on outcome of one trial
  - Emotional impact: massive (identity tightly coupled to result)
  - Exit logic: success OR failure. No middle ground
- Portfolio game: Many products, total capital compounds
  - Variance: high per-product, but low across portfolio
  - Expected value depends on infrastructure reusability and learning velocity
  - Emotional impact: medium (individual products matter less)
  - Exit logic: continue while slope is positive
- **Key difference:** "In single-bet, one miss feels existential. In portfolio, one miss is data."

#### 3.2 What Compound Systems Engineers Actually Build
- Not: individual products (those are the liquidity events)
- **Yes:** cognitive and technical capital
  - Reusable infrastructure (saves days/weeks per future product)
  - Observability harnesses (catch bugs automatically)
  - Testing frameworks (correctness by construction)
  - Agent orchestration systems (automation that scales)
  - Taste and judgment (decision-making capital)
- Show the internal view vs. external view:
  - External: "Not shipping" → "Why not just get a job?"
  - Internal: "Building cognitive capital, infrastructure, observability, taste"
  - External view is missing the leverage
- **Reframe:** "The output is not features. The output is capability."

#### 3.3 The Economics of Leverage
- Cost curve of products at each level:
  - Level 1: Every new product costs same as last (linear cost)
  - Level 2: New products inherit some infrastructure (sublinear cost)
  - Level 3: New products cost a fraction of Level 1 (exponential decay in cost)
- Visualize as a graph: products shipped (X-axis) vs. cost per product (Y-axis)
  - Level 1: flat line at high cost
  - Level 2: declining line (gradually improving)
  - Level 3: exponentially declining line (rapid improvement)
- The multiplier effect: "Normal engineer: 1×. Good engineer: 2×. Meta-engineer: 10× (and growing)."
- Why the multiplier works:
  - Every observability investment makes future debugging faster
  - Every testing framework makes future correctness cheaper
  - Every agent harness makes future automation cheaper
  - These don't degrade; they compound

#### 3.4 When Persistence Is Rational
- Persistence is NOT a moral virtue. It is a conditional strategy.
- Continue if and only if:
  1. Your iteration speed is increasing
  2. Your infrastructure is reusable
  3. Future experiments are cheaper than past ones
  4. Your downside is capped
  5. Your option space is expanding
- **Test for slope vs. intercept:**
  - Intercept: How much traction do you have right now? (Usually low)
  - Slope: Is leverage increasing? (This is what matters)
  - Mistake: Most people see low intercept and quit. They confuse low starting point with negative slope
  - **Correct logic:** "I care about slope. If slope is positive and downside is capped, I continue."

#### 3.5 The Risks of This Archetype (Honesty Required)
- This path is not heroic. It has real dangers.
- **Infinite Preparation Risk:** "I'm building leverage" becomes a story that hides fear of exposure
  - Mitigation: Hard review dates, forced shipping milestones, external reality checks
- **Cognitive Overfitting:** Building systems for problems that never arrive
  - Mitigation: Anchor infra to real use-cases, periodically prune abstractions, ask "What would break if this shipped tomorrow?"
- **Isolation Risk:** Few peers operate at this layer
  - Mitigation: Write doctrines (like this), seek high-signal conversations, avoid outcome-driven validation loops
- **Runway Erosion:** Leverage doesn't pay bills
  - Mitigation: Maintain baseline income, keep job option warm (not active), treat jobs as tools not identities
- **Exit Triggers (When to Get a Job):**
  - Runway drops below safety threshold
  - Learning slope flattens
  - Infrastructure stops generalizing
  - You've avoided shipping for >6-9 months
  - A job would increase future leverage (rare but possible)
- **What Would Be Failure:**
  - Sleepwalking into linearity (losing the slope)
  - Abandoning a positive-EV strategy too early
  - Confusing fear with prudence

---

### Section 4: The Shift to Systems Thinking
**Approximate length:** 1,000 words

#### 4.1 From Code to Systems
- The reframing that matters:
  - Instead of thinking about functions → think about bounded contexts
  - Instead of thinking about endpoints → think about service boundaries
  - Instead of thinking about code → think about invariants
  - Instead of thinking about tests → think about failure modes
  - Instead of thinking about logs → think about trace spans
- Example: Building an API
  - Level 1 thinking: "I need 5 endpoints: POST /users, GET /users/:id, POST /products, GET /products/:id, POST /orders"
  - Level 3 thinking: "I need bounded contexts: UserContext, ProductContext, OrderContext, each with invariants, contracts, and failure modes. Then I generate the endpoints."
- This shift is learnable. It's not intuition; it's pattern recognition.

#### 4.2 Constraints as the Unit of Design
- What meta-engineers build:
  1. **Environments** where constraints can be measured and enforced
     - Example: docker-compose with OTEL, Jaeger, Prometheus built-in
     - The environment itself enforces observability
  2. **Constraints** that capture what matters
     - Performance: p99 latency, max memory, throughput
     - Correctness: no data loss, atomic transactions, ordering preserved
     - Security: no SQL injection, auth required, rate limiting
  3. **Feedback loops** that prove constraints are met
     - Code change → tests → load tests → telemetry → constraint evaluation → pass/fail
     - If fail: agent fixes → retry
- The multiplier effect: "Build the constraint system once, then agents can verify it forever."

#### 4.3 Observability as Leverage
- Observability is not logging. Observability is the feedback loop.
- Three levels of observability:
  1. **Logs:** "Something happened"
  2. **Metrics:** "Here's the trend"
  3. **Traces:** "Here's how it happened" (observability)
- Traces + constraints = self-improving systems
  - System runs → trace data shows bottleneck → constraint captures it → agent fixes it → system runs again faster
- Example: "On session 1, you manually notice a database query is slow. On session 2, your observability harness catches it. On session 3, an agent auto-generates an index. On session 4, the system optimizes itself."

#### 4.4 The Compound Effect in Action
- Build observability harness → Cost: 1 week
- Harness catches bugs automatically → Cost: 0 (system saves time)
- Agent uses telemetry to self-fix → Cost: 0 (system saves time)
- System optimizes itself → Cost: 0
- End state: You're barely involved (but system is 10× faster than when you started)
- **The key realization:** "Every investment made in the system becomes a permanent leverage multiplier."

---

### Section 5: Why This Matters Now
**Approximate length:** 800 words

#### 5.1 The Economics of AI-Assisted Development
- AI (Claude, agents, code generation) is not about replacing engineers
- It's about shifting work up the stack:
  - Level 1: AI writes more code, faster (diminishing return)
  - Level 3: AI orchestrates systems, agents verify constraints, humans specify intent (exponential return)
- The compound effect with AI:
  - Without infrastructure: "Let me write this feature myself" → 2 days
  - With Level 3 infrastructure: "Let me specify the constraint and intent" → agent writes it → 2 hours
  - Cost difference: 16× faster
  - Multiply across 100 features: 1,600 hours saved per year
- **Who benefits from AI?** Engineers with strong systems thinking (Level 3). Everyone else gets commoditized.
- **Strategic implication:** "Invest in systems architecture and constraints now. That's your unfair advantage in an AI world."

#### 5.2 Systems Thinking vs. Product Thinking
- Product thinking: "How do I build this specific feature?"
  - Optimization: fast shipping
  - Failure mode: each feature costs as much as the last
- Systems thinking: "How do I build a system that can generate features like this?"
  - Optimization: compounding leverage
  - Success mode: each feature costs less than the last
- Both are valid games. They have different ROI curves.
- **The choice:** Which curve do you want to be on?

#### 5.3 Concrete Evidence: The Capital Accumulation
- Show the three levels with real metrics:
  - Level 1 engineer: 10,000 LOC/year, 70% rework, 3 critical bugs/quarter
  - Level 2 engineer: 8,000 LOC/year, 40% rework, 1 critical bug/quarter
  - Level 3 engineer: 3,000 LOC/year, 10% rework, 0 critical bugs/quarter (because agents catch them)
- The Level 3 engineer shipped LESS code but BUILT more (agents generated code)
- **The paradox:** "Fewer lines written. More product shipped. Better quality."
- This happens because of infrastructure and constraint enforcement, not heroic coding

#### 5.4 The Identity Shift Permission
- Old identity: "I am a developer who writes code"
  - Validation: code reviews, merge requests, shipped features
  - Status: junior to mid-level engineer
- New identity: "I am a systems engineer who designs self-improving systems"
  - Validation: constraint violations caught automatically, agents that ship features, systems that self-heal
  - Status: senior engineer, then principal, then leverage multiplier
- This shift is permanent once made
- **Permission statement:** "You don't need anyone's approval to make this shift. You just need to do the work."

---

## 5. Key Examples

### Example 1: The API Endpoint Evolution
- **Level 1:** Write a POST /users endpoint with validation, error handling, logging
- **Level 2:** Write a REST API generator that scaffolds CRUD for any entity
- **Level 3:** Write a constraint-based system where you specify:
  ```
  User {
    id: UUID
    email: email & unique
    createdAt: timestamp
    invariants: [email_unique, id_immutable]
  }
  ```
  And agents generate: schema, endpoints, tests, migrations, docs, monitoring

### Example 2: Bug Discovery Evolution
- **Level 1:** User reports bug → you debug locally → you fix → you ship
- **Level 2:** Automated tests catch some bugs → fewer user reports
- **Level 3:** Observability harness detects anomalies → agent proposes fix → system auto-verifies → fix ships without you
- Cost comparison: Level 1 (4 hours) vs. Level 3 (30 minutes agent setup + instant auto-fixes)

### Example 3: Feature Development Evolution
- **Product:** "We need a dashboard showing user engagement trends"
- **Level 1:** Build dashboard by hand → 1 week → one-off code
- **Level 2:** Build dashboard generator that reuses charting framework → 3 days → reusable patterns
- **Level 3:** Specify dashboard as constraints + agent generates it from schema → 30 minutes → composable with other systems
- **Multiplier:** Level 1 → Level 2 (2.3×). Level 2 → Level 3 (10×). Total: 23× faster

### Example 4: The Observation System
- Show a real system that demonstrates all three levels:
  - Level 1: Application code with println debugging
  - Level 2: Structured logging with ELK stack
  - Level 3: OTEL traces + Jaeger + Prometheus + constraint system + agent that reads traces and optimizes query plans

---

## 6. Diagrams Needed

### Diagram 1: The Three Levels of Engineering
- Title: "Leverage Curves: Where Engineers Operate"
- Y-axis: Leverage multiplier (1x to 100x)
- X-axis: Time invested (months)
- Three curves:
  - Level 1: flat at 1x (effort is effort)
  - Level 2: gradually increasing (some leverage, but sublinear)
  - Level 3: exponentially increasing (compound leverage)
- Annotation: "Where you are now" (Level 1) → "Where you can be" (Level 3)

### Diagram 2: Builder vs. Meta-Builder Comparison Table
- Two columns: Builder | Meta-Builder
- Six rows showing the shift (as in source material: CRUD endpoints → API generation systems, etc.)
- Use visual icons (e.g., code icon → gear icon) to show the conceptual shift

### Diagram 3: The Portfolio Game vs. Single-Bet Game
- Left side (Single-Bet):
  - One project box
  - Arrow to binary outcome: Success OR Failure
  - Caption: "High variance. One miss feels existential."
- Right side (Portfolio):
  - Five project boxes in a circle
  - Arrows flowing to central infrastructure box
  - Caption: "Medium variance per project. Infrastructure compounds."

### Diagram 4: The Cost Curve of Feature Development
- Title: "How Leverage Changes the Economics of Building"
- X-axis: Number of features shipped (0 to 100)
- Y-axis: Days per feature (0 to 10)
- Three curves:
  - Level 1: flat at 2-3 days
  - Level 2: declining to 1-2 days
  - Level 3: exponentially declining to 0.1-0.2 days (agents can build it)
- Area under curve shows cumulative time invested

### Diagram 5: The Skill Stack of Meta-Engineers
- Pyramid with seven layers (as in source material):
  - Base: Core Programming
  - Layer 2: Infrastructure as Code
  - Layer 3: Observability Engineering
  - Layer 4: Agent Orchestration
  - Layer 5: Architectural Design
  - Layer 6: Systems Thinking
  - Top: Mathematical Reasoning
- Annotation: "Most engineers develop only the bottom 2-3 layers. Meta-engineers develop all seven."

### Diagram 6: The Feedback Loop of Observability
- Title: "How Observability Creates Leverage"
- Circular flow:
  - Code change (top)
  - Automated tests (right)
  - Load tests (bottom-right)
  - Telemetry capture (bottom)
  - Constraint evaluation (left-bottom)
  - Agent fixes issues (left-top)
  - Back to code change
- Annotation: "This loop runs while you sleep."

---

## 7. Exercises

### Exercise 1: Identify Your Current Level
**Goal:** Readers assess their current position and understand the path forward.

**Instructions:**
1. Think about your most recent major project or feature.
2. Classify it:
   - Level 1: You wrote mostly custom code. Took you 2-4 weeks. Similar future projects will take the same time.
   - Level 2: You used frameworks/tools. Took you 1-2 weeks. Similar future projects will take slightly less time.
   - Level 3: You specified constraints and agents generated code. Took you 1-2 days. Similar future projects will take hours.
3. Write down: Where are you? Where do you want to be?
4. Identify one thing from Level 3 (constraints, observability, automation) that you could add to your next project.

**Expected output:** 1-page self-assessment + one concrete constraint or observability improvement to try next.

---

### Exercise 2: Map Your Leverage Curve
**Goal:** Readers understand the economics of their current approach and see where compounding happens.

**Instructions:**
1. List the last five projects/features you've built.
2. For each, estimate:
   - Time invested (in days)
   - Code reused from previous projects (%)
   - Infrastructure borrowed (yes/no)
   - Lessons applied (list 2-3)
3. Plot on a graph:
   - X-axis: Project number (1 to 5)
   - Y-axis: Time per project (in days)
4. Analyze your curve:
   - Flat? You're at Level 1 (each project is isolated)
   - Gradually declining? You're at Level 2 (some infrastructure reuse)
   - Rapidly declining? You're moving toward Level 3
5. Identify: What infrastructure/constraints would make project 6 cheaper?

**Expected output:** Curve graph + list of three infrastructure investments to try.

---

### Exercise 3: Audit Your Observability
**Goal:** Readers see where they have leverage today and where they're missing it.

**Instructions:**
1. Pick a system you built or maintain.
2. Answer these questions:
   - Can you find a performance bottleneck in < 5 minutes? (If no, logging is missing)
   - Can you see user behavior without asking users? (If no, events are missing)
   - Do you know when invariants break before users do? (If no, constraints are missing)
   - Can an agent understand your system from traces? (If no, observability is missing)
3. For each "no," identify what you'd need to add:
   - Structured logging? OTEL traces? Metrics? Constraint system?
4. Estimate cost vs. benefit:
   - Cost: days to implement
   - Benefit: hours saved per bug, per feature, per optimization
5. Prioritize: Which would give you the most leverage?

**Expected output:** Audit findings + prioritized list of observability improvements.

---

## 8. Cross-References

- **Chapter 2: "Building the Harness"** - Deep dive into the infrastructure layer (observability, constraints, environments)
- **Chapter 3: "Agents as Leverage"** - How to use AI and agents to amplify meta-engineer output
- **Chapter 4: "Liquidation Cadence"** - How to ship real products while maintaining infrastructure investments
- **Chapter 5: "Feedback Loops"** - How to design systems that measure and enforce constraints
- **Chapter 6: "Taste and Judgment"** - How capital accumulates beyond code (taste, decision-making, pattern recognition)
- **Appendix A: "Software Archetypes"** - Full comparison of indie hacker, lifestyle founder, career engineer, compound systems engineer
- **Appendix B: "Compound Systems Engineer Doctrine"** - Full text of the doctrine for reference

---

## 9. Word Count Target

- **Total Chapter:** 5,500-7,000 words
  - Section 1 (The Problem): 800 words
  - Section 2 (The Archetype): 1,200 words
  - Section 3 (The Game): 1,200 words
  - Section 4 (Systems Thinking): 1,000 words
  - Section 5 (Why This Matters Now): 800 words
  - Diagrams + Examples: 500 words (mostly captions and brief descriptions)

---

## 10. Status

**Draft** - Ready for author review and feedback

---

## Notes for Author/Editor

1. **Tone:** This chapter is foundational. It should be honest about tradeoffs (not inspirational), grounded in economics (not motivation), and practical (not theoretical). The reader should feel "this makes sense" not "this is cool."

2. **Key Moments:**
   - Section 2.2 (The Three Levels): This is the mental model moment. Readers should recognize themselves in Level 1 and see Level 3 as reachable.
   - Section 3.4 (Persistence is Rational): This is the permission moment. Readers should feel empowered to persist or stop based on slope, not shame.
   - Section 5.1 (AI Economics): This is the modern relevance moment. Readers should see why this archetype matters more now than before.

3. **Common Objections (Anticipate in Text):**
   - "This sounds like overthinking. Why not just ship?"
     - **Answer:** Because shipping without leverage creates more work, not less. One feature at Level 1 takes 2 weeks. One feature at Level 3 takes 2 days. Long-term, Level 3 wins.
   - "Doesn't this require a large team?"
     - **Answer:** No. Meta-engineers often work solo. That's the whole point. Better infrastructure = fewer people needed.
   - "What if I'm at a job? Can I still do this?"
     - **Answer:** Yes. Many meta-engineers operate within jobs. They build leverage (infrastructure, observability, constraints) as part of their daily work. The archetype is about how you think, not your employment status.

4. **Examples to Potentially Add:**
   - Real code example: A constraint-based system (3-4 lines of constraint definition vs. 50 lines of manual validation)
   - Metric example: Actual improvement curves (e.g., "Day 1 to ship: 10 days → Day 30 to ship: 3 days")
   - Interview/conversation: Quote from a meta-engineer (published, like Dan Luu or Charity Majors on systems thinking)

5. **Missing from PRD (Optional but Valuable):**
   - A brief history of where this archetype comes from (systems thinking, capital allocation, startup lessons)
   - A "what's changed" section (why this matters more in 2026 than 2016)
   - A case study (one person's journey from Level 1 → 3, what changed, what they built)

---

## Version History

| Date | Author | Status | Notes |
|------|--------|--------|-------|
| Jan 26, 2026 | Claude | Draft | Initial PRD based on source materials |
