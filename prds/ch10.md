# Chapter 10 PRD: Building the Harness

**Status**: Draft
**Author**: James Phoenix
**Created**: January 26, 2026
**Chapter**: 10
**Book**: Compound Engineering: Infrastructure That Compounds

---

## 1. Overview

Chapter 10 teaches readers how to build production infrastructure around coding agents, treating AI code generation not as isolated feature-building, but as a controlled system requiring multiple layers of harnesses. This chapter covers the four-layer harness architecture (Claude Code configuration, repository engineering, meta-engineering, and closed-loop optimization), shows how to build factories rather than just products, and demonstrates how infrastructure itself becomes capital that compounds over time. Readers will understand that the LLM is the least controllable part of any system, and everything else is engineering, infrastructure to constrain, measure, and improve the signal-to-noise ratio of AI-assisted development.

**Core thesis**: You don't build better prompts. You build better harnesses. Each layer amplifies signal and attenuates noise until the system reliably produces high-quality code without human intervention.

---

## 2. Learning Objectives

After completing Chapter 10, readers will be able to:

1. **Architect multi-layer harnesses**: Understand the four-layer harness model (Claude Code → Repository → Meta-Engineering → Closed-Loop) and recognize which layer to optimize for different problems.

2. **Build infrastructure as capital**: Shift mindset from "using AI to build features" (linear) to "using AI to build infrastructure that builds features" (exponential), recognizing the ROI of meta-infrastructure investments.

3. **Instrument systems for closed-loop optimization**: Implement telemetry-driven code quality loops using OTEL and constraint specifications to automatically detect and fix regressions.

4. **Design queryable project context with MCP**: Build custom MCP servers that expose architecture graphs, pattern examples, test coverage, and git history as on-demand knowledge resources for AI agents.

5. **Orchestrate long-running agents**: Implement two-agent patterns, progress tracking, feature lists, and browser automation to handle multi-session development workflows reliably.

6. **Measure infrastructure multipliers**: Define metrics for automation coverage, tool ROI, and infrastructure leverage to justify meta-infrastructure investment.

---

## 3. Source Articles

### Primary Sources
- `building-the-harness.md` - The four-layer harness model and signal processing mental framework
- `building-the-factory.md` - Meta-infrastructure, automation levels, and self-improving systems
- `closed-loop-telemetry-driven-optimization.md` - Telemetry as active feedback control and constraint-driven refactoring
- `mcp-server-project-context.md` - Queryable project knowledge with custom MCP servers
- `infrastructure-principles.md` - Compound systems identity and boundary contracts

### Supplementary Sources (Added from KB Analysis Jan 27, 2026)
- `ralph-loop-patterns.md` - RALPH loop implementation for long-running agent sessions
- `parallel-agents-for-monorepos.md` - Git worktree patterns for parallel agent execution
- `agent-sdk-patterns.md` - Using Agent SDK v2-preview for programmatic orchestration
- `plan-mode-workflow.md` - Plan mode for complex multi-step implementations
- `constraint-specification-patterns.md` (TO BE CREATED) - How to define and validate performance constraints

### Additional Sources (Added from KB Analysis Jan 28, 2026)
- `ci-cd-agent-patterns.md` - GitHub Actions patterns for agent verification, nightly RALPH loops, response caching, cost budget enforcement, behavioral regression tests, multi-model testing
- `agent-memory-patterns.md` - Checkpoint/resume patterns, three-tier memory hierarchy (session, file-based, event-sourced), RALPH-compatible memory, webhook integration for human approval
- `ai-cost-protection-timeouts.md` - Multi-layer timeout limits (job-level, request-level, input-level), budget alerts, model selection for cost efficiency
- `checkpoint-commit-patterns.md` - Git strategies for AI-assisted development, commit timing patterns, branch strategies, recovery patterns
- `agent-reliability-chasm.md` - Four-turn framework (understand, decide, execute, verify), pre-action checks, post-action verification, reliability stack
- `12-factor-agents.md` - 12 factors for production-ready agents: natural language to tool calls, own your prompts, own context window, tools as structured outputs, unified state, pause/resume, contact humans with tools, own control flow, compact errors, small focused agents, trigger from anywhere, stateless reducer

---

## 4. Detailed Outline

### 4.1 The Mental Model: Signal Processing

**Section Goal**: Establish the foundational framework readers will use throughout the chapter.

- **4.1.1 The LLM as a Noisy Channel**
  - Definition: LLMs are probabilistic systems that output high variance
  - Each layer of harness increases signal-to-noise ratio
  - Control theory framing: setpoint (goals) → controller (harness) → plant (LLM)
  - Metaphor: Radio receiver with antenna (capture), filters (layers), tuner (constraints)

- **4.1.2 The Four Layers**
  - Visual: Concentric rings diagram with LLM at center
  - Layer 1 (Claude Code): Raw harness around LLM
  - Layer 2 (Repository): Environmental clarity through OTEL, tests, DDD
  - Layer 3 (Meta-Engineering): Process automation and orchestration
  - Layer 4 (Closed-Loop): Self-correction and constraint-driven optimization
  - Each layer's contribution to signal quality

- **4.1.3 The Fundamental Insight**
  - Quote: "The LLM is the least controllable part of the system. Everything else is engineering."
  - Implication: Resources spent on harness > resources spent on better prompts
  - Practical corollary: If something fails 30%, don't write 100 prompts; build 1 layer that constrains to success

---

### 4.2 Layer 1: Configuring Claude Code

**Section Goal**: Teach readers how to configure the innermost harness, Claude Code itself.

- **4.2.1 CLAUDE.md as Agent Specification**
  - Components of a strong claude.md: WHAT, WHY, HOW structure
  - Providing business context vs. technical context
  - How CLAUDE.md constrains the solution space
  - Example: Authentication system claude.md with architectural invariants

- **4.2.2 Claude Code Hooks**
  - Pre-processing hooks: linting, formatting, environment setup
  - Post-processing hooks: test validation, type checking, commit validation
  - Hook composition for pipeline reliability
  - Example: Pre-commit hook that runs tests and rejects broken code

- **4.2.3 Scoping and Constraints**
  - Defining the scope (what files the agent can touch)
  - Hard constraints: "Only modify src/features/, not src/core/"
  - Quality gates: Tests must pass before suggesting changes
  - Example: Feature implementation scope definition

- **4.2.4 Long-Running Agent Sessions**
  - Two-agent pattern: Initializer (setup) + Coding Agent (work)
  - Session initialization sequence: health checks → progress review → feature selection
  - Progress files as handoff mechanism between sessions
  - Feature lists in JSON (structured, resistant to premature victory)
  - Example: Session bootstrap script and feature tracker

---

### 4.3 Layer 2: Repository Engineering

**Section Goal**: Teach how environment design multiplies AI effectiveness.

- **4.3.1 Observability Stack (OTEL + Jaeger)**
  - Why observability is not just monitoring
  - OTEL setup for capturing metrics, logs, traces
  - Jaeger for visualizing distributed traces
  - Example: Docker Compose setup with Jaeger + OTEL collector
  - Trace propagation across service boundaries

- **4.3.2 Testing Infrastructure**
  - Context-efficient test runners that provide clear signal
  - Test output compression: silence on success, detail on failure
  - Multiple test levels: unit, integration, E2E, load
  - Example: Test runner that uses backpressure to reduce noise
  - Browser automation for realistic verification (Playwright/Puppeteer)

- **4.3.3 Prod/Dev Parity**
  - Why environment parity matters for AI-generated code
  - Dockerization for consistency
  - Docker Compose for local multi-service development
  - Example: Docker setup ensuring same Node version, package lock consistency
  - Database seeding for realistic test data

- **4.3.4 Code Structure (Domain-Driven Design)**
  - DDD layering: domain → application → infrastructure → presentation
  - Why DDD helps AI agents: Clear responsibilities, explicit boundaries
  - Example: E-commerce system with clean DDD structure
  - How DDD reduces hallucination by providing structural guardrails

- **4.3.5 Dependency and Import Constraints**
  - Preventing circular dependencies
  - Clear module boundaries
  - Tools for enforcing: eslint-plugin-boundary, depcheck
  - Example: Lint rules that prevent application layer importing infrastructure directly

---

### 4.4 Layer 3: Meta-Engineering

**Section Goal**: Teach automation of the automation process itself.

- **4.4.1 Claude Code Scripts and Workflows**
  - Workflow-specific agent prompts in `.claude/commands/`
  - Example commands: implement-feature, fix-failing-test, review-pr, refactor-module
  - How to compose complex tasks from simpler Claude Code invocations
  - Example: Multi-step feature implementation workflow

- **4.4.2 Tests for Tests**
  - Meta-testing: verify your test infrastructure works
  - Example: Testing that your test runner correctly captures failures
  - Testing that linters run correctly
  - Testing that Docker environment setup is reproducible
  - Example: Test suite that verifies backpressure compression works

- **4.4.3 Tests for Telemetry**
  - Verify observability before you need it in production
  - Example: Test that OTEL trace propagation works through all services
  - Span correlation tests
  - Metric cardinality tests
  - Example: CI job that validates observability setup

- **4.4.4 Agent Swarm Tactics**
  - Parallel agent execution for large tasks
  - Breaking down specs into independent subtasks
  - Merging results and resolving conflicts
  - Example: Five agents implementing different modules in parallel
  - Coordination patterns: merge strategies, conflict detection

- **4.4.5 Nightly Job Orchestration**
  - Automating repeating tasks that Claude Code executes
  - Examples: Dependency updates, performance benchmarks, security audits
  - Job configuration with success/failure hooks
  - Creating PRs, issues, and Slack notifications from job results
  - Example: Nightly orchestrator config with multiple jobs

---

### 4.5 Layer 4: Closed-Loop Optimization

**Section Goal**: Teach self-improving infrastructure that optimizes itself automatically.

- **4.5.1 Telemetry as Control Input**
  - Shift from passive monitoring to active feedback control
  - Control theory framework: setpoint (constraints) → controller (agent) → plant (service)
  - Constraint violations trigger automated refactoring
  - Example: Memory constraint violation → agent proposes fix → re-run tests
  - Closed-loop diagram with feedback

- **4.5.2 Constraint Specification**
  - Defining performance constraints as mathematical invariants
  - Memory constraints: max MB, tolerance percentage, sustained load window
  - Heap constraints: max retained growth slope, observation window
  - Latency constraints: p99, p90, p50 max milliseconds
  - Cardinality constraints: max unique labels, growth bounds
  - Error budgets: acceptable failure rates
  - Example: Complete constraints.yaml for a production service

- **4.5.3 The Agent Optimization Loop**
  - Generate diagnostics from constraint violations
  - Infer root causes (use profiler output, traces)
  - Agent proposes targeted refactoring
  - Apply patch and re-run tests + load test
  - Score against constraints
  - Loop until pass or escalate to human
  - Example: Memory leak detection and automated fix

- **4.5.4 CI/CD Integration**
  - Optimization loop as part of GitHub Actions workflow
  - Triggered on: push to main, nightly schedule
  - Steps: run load tests → capture telemetry → evaluate constraints → spawn optimizer agent if violations
  - Auto-commit improvements to main
  - Example: Complete GitHub Actions workflow

- **4.5.5 Example: Memory Leak Optimization**
  - Real scenario: Heap retained growth over 20 minutes
  - Constraint: heap.retained_growth_slope = 0
  - Agent diagnosis: Event list accumulation without cleanup
  - Agent fix: Add `.clear()` after batch processing
  - Verification: Re-run load test, confirm slope = 0

- **4.5.6 Why This Is Novel**
  - Merges control theory + observability + code generation + constraint solving
  - Nobody uses telemetry as active control input for automatic optimization
  - This becomes standard in 3-5 years (today → agent fixes failures → auto-merges)
  - Your career advantage: Building this now

---

### 4.6 Building the Factory, Not Just the Product

**Section Goal**: Teach the exponential mindset shift from features to infrastructure.

- **4.6.1 The Problem: Linear Productivity**
  - Day N: Use AI to build Feature N (8 hours saved)
  - Total: 8 hours × N days
  - Opportunity cost: Each day on features is a day not building tools
  - Calculation example: 100 features × 2 hours = 200 hours
  - vs. 10 hours to build tool + 100 × 0.1 hours = 20 hours saved = 180 hours (90% reduction)

- **4.6.2 The Solution: Four Levels of Automation**
  - Level 0: Manual coding (1x productivity)
  - Level 1: AI-assisted features (5-10x). Most developers stop here
  - Level 2: Tools that generate code (20-50x)
  - Level 3: Meta-infrastructure (100-500x)
  - Growth chart showing exponential curve

- **4.6.3 Identifying High-Leverage Infrastructure**
  - Time tracking method: What takes longest? What's most repetitive? What's most frustrating?
  - Value calculation: (Time per task) × (Frequency) × (Automation %)
  - Example: API endpoints taking 30 min × 10/week × 90% automation = 270 min/week savings
  - ROI calculation: 4 hours build time / 4.5 hours/week savings = payback in 0.9 weeks

- **4.6.4 Four High-Leverage Infrastructure Categories**
  - Category 1: Custom MCP Servers
    - CMS integration servers
    - Performance tracking servers
    - Code generation servers
    - Database migration servers
    - Example: Article generation MCP server saving 42 hours across 90 articles

  - Category 2: Development Bootstrap Tools
    - Project scaffolders
    - Feature generators (model + API + UI + tests)
    - Integration setup (databases, auth, monitoring)
    - Example: One-command project setup reducing 4 hours to 3 minutes

  - Category 3: Evaluation Suite Infrastructure
    - Visual regression testing
    - Performance monitoring dashboards
    - Dependency auditing
    - Quality metrics dashboards
    - Example: Playwright visual regression catching UI regressions automatically

  - Category 4: Claude Code Orchestration
    - Nightly job runners
    - Automatic issue assignment
    - Progress monitoring
    - Auto-review systems
    - Example: GitHub issue → Claude Code → PR → Auto-merge on tests passing

- **4.6.5 The Compounding Effect**
  - Week 1: One tool saves 4 hours/week (net: -8 hours investment)
  - Week 2: Three tools save 6 hours/week (net: +42 hours that week)
  - Week 4: Ten tools save 8 hours/day (most work automated)
  - Month 2: Meta-tools building tools (exponential growth)
  - Month 6: Self-improving infrastructure (autonomous operation)
  - Visual: Exponential growth curve over 6 months

- **4.6.6 Universal Code Generator Framework**
  - Pattern: All generators follow template → variable substitution → file creation
  - Meta-pattern: Build a generator factory
  - Example: YAML template-based generator
  - Now new generators are 95% faster (just write template, no code)

- **4.6.7 Self-Improving Infrastructure**
  - Monitor usage → Detect inefficiencies → Generate improvement tickets
  - Auto-assign to Claude Code → Implement fixes → Auto-deploy
  - System improves itself autonomously
  - Example: Tool monitoring its own usage and suggesting optimizations

---

### 4.7 Queryable Project Context with MCP Servers

**Section Goal**: Teach how to make project knowledge dynamic, not static.

- **4.7.1 The Problem with Static Documentation**
  - CLAUDE.md limitations: Can't answer specific queries dynamically
  - Information staleness: Documents written January, codebase in November
  - All-or-nothing context: Either load everything (15K tokens, 90% unused) or minimal (2K tokens, often insufficient)
  - Can't provide real-time data: recent changes, test coverage, performance metrics

- **4.7.2 The MCP Protocol Solution**
  - MCP (Model Context Protocol) as standard for AI-to-data connections
  - AI agent queries specific resources instead of loading all docs
  - Resources concept: queryable knowledge items with URIs
  - Diagram: AI Agent → MCP Protocol → Custom MCP Server → Project Codebase
  - Example queries: "architecture-graph://auth", "pattern-examples://factory-functions"

- **4.7.3 Defining Resources**
  - Resource URI patterns and semantic naming
  - Seven key resource types:
    - architecture-graph://project (dependency graph)
    - architecture-graph://{module} (module-specific)
    - pattern-examples://{pattern} (real code examples)
    - recent-changes://last-{period} (git history)
    - test-coverage://{module} (test metrics)
    - performance-metrics://{endpoint} (runtime stats)
    - code-search://{query} (AST-aware search)
  - JSON resource definition schema

- **4.7.4 Building the MCP Server**
  - Server structure: list resources handler + read resource handler
  - TypeScript SDK setup
  - Example: Complete MCP server with all handler types
  - Server lifecycle and connection management

- **4.7.5 Resource Analyzers**
  - Architecture analyzer: Using ts-morph for AST analysis
    - Nodes: modules, classes, functions
    - Edges: import, call, extends relationships
    - Filtering for specific modules or full project
    - Example: Complete architecture analyzer implementation

  - Pattern examples finder: Identifying code patterns
    - Factory functions (functions starting with 'create')
    - Result types (Result<T, E> patterns)
    - Async/await patterns
    - Example: Factory function finder with line numbers

  - Git history analyzer: Recent changes
    - Commit info: author, date, message
    - Files changed per commit
    - Period parsing ('last-week', 'last-month')
    - Example: Git analyzer with period filtering

  - Coverage analyzer: Test metrics
    - Module coverage percentages
    - Statements, branches, functions covered
    - Identifying low-coverage areas

  - Performance analyzer: Runtime metrics
    - API endpoint latencies (p50, p95, p99)
    - Error rates
    - Throughput metrics

- **4.7.6 Integration with Claude Code**
  - MCP server configuration in Claude Code settings
  - Using resources in prompts: "Query architecture-graph://auth before implementing..."
  - Example: Refactoring workflow that queries before starting

- **4.7.7 Five Key Use Cases**
  - Use Case 1: Understanding dependencies
    - Query: "What functions call authenticate()?"
    - MCP advantage: Dynamic graph traversal vs static documentation

  - Use Case 2: Finding similar code patterns
    - Query: "Show me how other services handle errors"
    - MCP advantage: Real examples from current codebase vs generic patterns

  - Use Case 3: Impact analysis
    - Query: "What changed in API layer this week?"
    - MCP advantage: Live git history vs manual tracking

  - Use Case 4: Identifying coverage gaps
    - Query: "Which modules need more tests?"
    - MCP advantage: Real-time metrics vs stale documentation

  - Use Case 5: Performance optimization
    - Query: "Which API endpoints are slowest?"
    - MCP advantage: Live metrics vs no production data in docs

- **4.7.8 Best Practices**
  - Caching expensive queries (5-minute TTL)
  - Providing multiple granularities (low/medium/high detail)
  - Including documentation in responses
  - Supporting filters and parameters
  - Combining with static documentation (CLAUDE.md for principles, MCP for examples)
  - Filtering sensitive data (API keys, secrets, PII)

- **4.7.9 Common Pitfalls**
  - Slow queries: Re-analyzing entire codebase on every request (solution: cache)
  - Exposing sensitive data: Returning secrets in responses (solution: sanitize)
  - Returning too much data: 10MB JSON responses (solution: paginate)
  - No error handling: Crashes on invalid queries (solution: validate inputs)

---

### 4.8 Bringing It Together: A Complete Example

**Section Goal**: Show how all four layers work together on a realistic scenario.

- **4.8.1 Scenario: Building a Payment Processing Module**
  - Requirements: Stripe integration with real-time monitoring
  - Constraints: <100ms latency p99, <300MB peak memory, 99.95% uptime
  - Starting point: Skeleton API, no tests, no observability

- **4.8.2 Layer 1: Configure Claude Code**
  - claude.md with: Stripe API contract, error handling patterns, payment invariants
  - Scope: Only touch `src/payments/`
  - Hooks: Pre-commit linting, post-commit test validation
  - Session bootstrap: Review progress, select next feature

- **4.8.3 Layer 2: Set Up Repository Engineering**
  - OTEL instrumentation for payment operations
  - Jaeger for visualizing payment flows
  - Test suite: Unit tests (Stripe mocking), integration tests (real test keys), E2E tests (payment flow)
  - Docker setup with local Stripe mock
  - DDD structure: domain/payment entities → application/payment services → infrastructure/stripe-client
  - Dependency rules: API handlers only call application services

- **4.8.4 Layer 3: Build Meta-Engineering**
  - Claude Code script: "implement-payment-feature.md" breaking down requirements
  - Tests for tests: Verify mock Stripe works, latency measurement works
  - Tests for telemetry: Verify trace propagation through payment flow
  - Parallel agents: One building Stripe client, one building handlers, one building tests

- **4.8.5 Layer 4: Enable Closed-Loop Optimization**
  - Constraints yaml: p99 latency <100ms, memory <300MB, heap slope = 0
  - Nightly load test: 1000 payments/second for 15 minutes
  - Capture telemetry from load test
  - If p99 latency > 100ms: Spawn optimizer agent
  - Agent identifies bottleneck (batch sizes), proposes fix, re-tests
  - Example: Batch size optimization reducing p99 from 145ms to 87ms

- **4.8.6 Layer 2.5: Add MCP Server for Context**
  - Custom MCP exposing: payment patterns, error handling examples, recent changes
  - Query: "Show me all error handling patterns in payments module"
  - Agent uses examples to implement consistent error handling
  - Query: "What changed in payments last week?"
  - Agent understands recent decisions and builds compatible code

- **4.8.7 Results**
  - Feature development: 2 weeks (vs 6 weeks manual)
  - Production regressions: 0 (vs 3 bugs in manual version)
  - Maintenance time: Nightly optimization runs automatically
  - Knowledge: MCP server queries give new team members instant context

---

### 4.9 Measuring Success

**Section Goal**: Teach how to quantify infrastructure investment ROI.

- **4.9.1 Five Key Metrics**
  - Automation Coverage: (Automated tasks) / (Total recurring tasks)
    - Target: >70% within 3 months
    - Tracking method: Maintain list of recurring tasks, mark automated

  - Time Savings: Σ(Task time × Frequency × Automation %)
    - Target: >20 hours/week within 2 months
    - Tracking: Before/after time logs

  - Tool ROI: (Time saved) / (Build time)
    - Target: >5x within 1 month of building
    - Tracking: Build time vs cumulative time saved

  - Infrastructure Leverage: (Output with tools) / (Output without tools)
    - Target: >10x within 6 months
    - Tracking: Features shipped per week before/after

  - Tool Reuse: (Tools used by others) / (Total tools)
    - Target: >30% (tools useful beyond just you)
    - Tracking: Usage metrics across team

- **4.9.2 Dashboard Creation**
  - Weekly metrics review ritual
  - Tools underutilized? Deprecate or improve
  - Patterns emerging? Build meta-tools
  - High ROI tools? Expand capabilities

---

### 4.10 Common Pitfalls and How to Avoid Them

**Section Goal**: Prepare readers for obstacles.

- **4.10.1 Building Before Validating Need**
  - Pitfall: Spending weeks on tool for task that happens 2x/year
  - Solution: Track frequency before building. Only build if >3 times/month.

- **4.10.2 Over-Engineering**
  - Pitfall: Spending 40 hours building perfect tool for 10-hour problem
  - Solution: "80% solution in 20% time" rule. Build fast, iterate based on usage.

- **4.10.3 Building Without Using**
  - Pitfall: Tool sits unused while team continues manual process
  - Solution: Force adoption. Delete manual process. Make tool the only way.

- **4.10.4 Not Documenting**
  - Pitfall: Build tool, forget how to use it after 2 weeks
  - Solution: Write README with examples BEFORE building. Claude Code can generate both.

- **4.10.5 Single-Use Tools**
  - Pitfall: Tool only works for one specific case
  - Solution: Add configuration. Make tool flexible and composable.

- **4.10.6 Slow Optimization Loops**
  - Pitfall: Optimization takes 30 minutes per constraint violation
  - Solution: Parallelize. Use better analyzers. Cache expensive computations.

- **4.10.7 Constraint Specification Problems**
  - Pitfall: Constraints too tight (optimizer never passes) or too loose (no optimization)
  - Solution: Iteratively calibrate based on real load tests. Start loose, tighten over time.

---

### 4.11 The Harness Maturity Progression

**Section Goal**: Show readers the realistic arc of implementation.

- **4.11.1 Week 1: Layer 1 (Claude Code)**
  - Write strong claude.md
  - Add pre/post hooks
  - First feature built with constraints
  - Outcome: 3x faster feature development

- **4.11.2 Week 2-3: Layer 2 (Repository Engineering)**
  - Add OTEL instrumentation
  - Set up Jaeger
  - Add comprehensive test suite
  - DDD refactoring
  - Outcome: Better signal, fewer regressions

- **4.11.3 Month 1: Layer 3 (Meta-Engineering)**
  - Claude Code workflows for common tasks
  - Tests for tests and telemetry
  - Nightly job orchestration
  - Outcome: Work happens largely autonomously

- **4.11.4 Month 2-3: Layer 4 (Closed-Loop Optimization)**
  - Define constraints
  - Build optimization loop
  - First automated optimization runs
  - Outcome: Zero-touch performance maintenance

- **4.11.5 Month 4+: The Factory**
  - MCP servers for project context
  - Meta-tools (generators of generators)
  - Self-improving infrastructure
  - Outcome: Exponential productivity

---

## 5. Key Examples

The chapter requires the following code examples and scenarios:

### 5.1 Configuration Examples
- Complete strong claude.md for payment module
- claude.md hooks (pre-commit, post-edit)
- constraints.yaml with memory, latency, error budget constraints
- nightly-jobs.config.ts with job definitions
- mcp-servers.json configuration

### 5.2 Architecture Examples
- DDD layered architecture (domain → application → infrastructure → presentation)
- Docker Compose setup with Jaeger, OTEL collector, service
- GitHub Actions workflow for optimization loop
- Database schema with migration tracking

### 5.3 Implementation Examples
- Test runner with backpressure compression
- MCP server with 4 resource handlers
- Architecture analyzer using ts-morph
- Pattern finder for factory functions
- Git history analyzer
- Optimization agent loop

### 5.4 Workflow Examples
- Two-agent pattern: initializer + coder
- Session bootstrap sequence
- Feature list JSON format
- Progress file template
- Complete refactoring workflow with MCP queries

### 5.5 Real-World Scenarios
- Memory leak detection and automated fix
- API endpoint latency optimization
- Test coverage gap identification
- Dependency cycle breakage
- Constraint violation → agent fix → re-test cycle

### 5.6 Anti-Patterns
- Overly loose constraints (optimizer never finishes)
- Overly tight constraints (optimizer always escalates)
- Stale static documentation
- Optimization loops too slow
- Single-use tools
- Infrastructure built before validating need

---

## 6. Diagrams Needed

1. **Four-Layer Harness Architecture Diagram**
   - Concentric rings: LLM at center, surrounded by Claude Code, Repository, Meta-Engineering, Closed-Loop
   - Signal amplification annotations on each layer
   - Label: "Signal-to-Noise Ratio Increases Outward"

2. **Signal Processing Mental Model**
   - Input → Meta-Engineering → Repository → Claude Code → LLM → Output
   - Show noise attenuation at each layer
   - Radio receiver analogy

3. **Control Theory Feedback Loop**
   - Constraints (setpoint) → Optimizer Agent (controller) → Code (plant)
   - Telemetry sensor → Constraint evaluation
   - Closed-loop arrow back to constraints

4. **MCP Server Architecture**
   - AI Agent box → MCP Protocol arrow → Custom MCP Server → Project Codebase
   - Show resource URIs inside server box
   - Show analyzers inside server

5. **Automation Levels Progression Chart**
   - Y-axis: Productivity multiplier (1x to 500x)
   - X-axis: Implementation phase (Level 0 → 1 → 2 → 3)
   - Exponential curve
   - Label key milestones

6. **Optimization Loop Timeline**
   - Horizontal timeline showing: Run load test → Capture telemetry → Evaluate constraints → (decision diamond) → Escalate or Loop
   - Show metric snapshots at each point
   - Example constraint violation and fix

7. **Repository DDD Architecture**
   - Layer diagram: Presentation → Application → Domain → Infrastructure
   - Arrows showing allowed dependencies (inward only)
   - Example: API Handler → Service → Entity → Stripe Client

8. **Compounding Effect Over 6 Months**
   - Stacked area chart
   - Week 1: Tool 1 contribution
   - Week 2-4: Tools 2-10 adding incrementally
   - Month 2: Meta-tools creating exponential jump
   - Month 6: Self-improving infrastructure plateau
   - Y-axis: Hours/week saved

9. **Telemetry Constraint Specification**
   - YAML constraints file with annotations
   - Show: memory_max_mb, heap_growth_slope, p99_latency_ms, error_budget_percent
   - Example values and why they matter

10. **MCP Resource Query Examples**
    - Show 5 different resource URIs
    - Example responses for each
    - How agent uses response in implementation

---

## 7. Exercises

### Exercise 1: Design Your Harness (Practical, 60-90 minutes)

**Objective**: Apply the four-layer harness model to readers' own projects.

**Instructions**:

1. **Pick a failing system**: Identify one system in your codebase that's buggy or slow (or create a hypothetical).

2. **Diagnose signal quality**:
   - Layer 1: Is claude.md strong? Do hooks run? Document current state.
   - Layer 2: Does the repo have tests? OTEL? DDD structure? Document gaps.
   - Layer 3: Are there automation scripts? Tests for tests? Document missing pieces.
   - Layer 4: Are there constraints? Closed-loop optimization? Document needs.

3. **Propose improvements**:
   - For each layer, write one specific improvement (2-3 sentences each)
   - Prioritize: Which layer gives 10x improvement for least effort?
   - Start there.

4. **Implement one improvement**:
   - Pick the highest-leverage layer improvement from step 3
   - Implement it (code + tests)
   - Measure before/after: What metric improved?

5. **Document**:
   - Write a brief reflection: "My harness before/after"
   - What surprised you?
   - What would you do differently next time?

**Deliverable**: Brief writeup + implemented improvement + before/after metrics

---

### Exercise 2: Build an MCP Server (Hands-On, 120-180 minutes)

**Objective**: Experience dynamic project context retrieval.

**Instructions**:

1. **Define Resources**:
   - Pick 3 resource types you want your MCP server to expose
   - For each, write the URI pattern and description
   - Example: architecture-graph://module, recent-changes://last-week, test-coverage://module

2. **Implement Analyzers**:
   - Build one analyzer (your choice): architecture, patterns, git history, or coverage
   - Implement using provided code examples as template
   - Add tests for your analyzer

3. **Build MCP Server**:
   - Create server using TypeScript MCP SDK
   - Hook up your analyzer
   - Test locally: can Claude read your resources?

4. **Test Queries**:
   - Query your MCP server 3 different ways
   - Show that agent can use responses to make better decisions
   - Example: Agent writes code that matches existing patterns (verified via MCP)

5. **Measure**:
   - Before MCP: How much static context did you need?
   - After MCP: How much context does agent actually query?
   - Context reduction percentage?

**Deliverable**: Working MCP server + test queries + measurement data

---

### Exercise 3: Build a Nightly Optimization Loop (Advanced, 180-240 minutes)

**Objective**: Implement closed-loop telemetry-driven optimization.

**Instructions**:

1. **Define Constraints**:
   - Pick a real system you want to optimize (or create test service)
   - Define 2-3 constraints (memory, latency, error rate)
   - Write constraints.yaml

2. **Build Load Test**:
   - Create realistic load test that exercises your system
   - Capture metrics during test (via OTEL, Prometheus, or simple timing)
   - Store metrics as JSON

3. **Implement Constraint Checker**:
   - Script that reads metrics.json + constraints.yaml
   - Evaluates each constraint
   - Returns violations + severity

4. **Build Agent Prompt**:
   - Write .claude/agents/optimizer.md
   - Agent receives: violations, metrics, codebase
   - Agent proposes 1-2 minimal refactors
   - Agent applies changes + re-runs tests

5. **Wire Into CI/CD**:
   - GitHub Actions workflow that:
     - Runs load test nightly
     - Captures metrics
     - Checks constraints
     - Spawns optimizer agent if violations
     - Creates PR if fixes found
   - Test with 3 different scenarios (2 violations, 1 pass, 1 escalate)

6. **Measure**:
   - Before: Manual debugging + fixing took X hours
   - After: Automated loop runs nightly, no human time
   - What's the ROI of building this?

**Deliverable**: Working optimization loop + constraints.yaml + agent prompt + CI/CD workflow + measurement

---

## 8. Cross-References

### Internal Book References
- Chapter 1: "Why Compound Engineering": Foundation for harness philosophy
- Chapter 3: "The Signal Processing View": Deep dive into layers
- Chapter 5: "Testing Infrastructure": Relates to Layer 2
- Chapter 8: "Claude Code Configuration": Foundation for Layer 1
- Chapter 9: "Context Engineering": Leads into MCP servers
- Chapter 11: "Measurement and Iteration": Metrics from Section 4.9

### External Resources
- Model Context Protocol Documentation: modelcontextprotocol.io
- Anthropic Engineering: "Effective Harnesses for Long-Running Agents"
- OpenTelemetry Documentation: opentelemetry.io
- Domain-Driven Design by Eric Evans (book reference)
- Control Theory Fundamentals (math reference)
- GitHub Actions Documentation
- Playwright Documentation (browser automation)

### Related Concepts in Knowledge Base
- 01-Compound-Engineering/writing-a-good-claude-md
- 01-Compound-Engineering/context-efficient-backpressure
- 01-Compound-Engineering/12-factor-agents
- 01-Compound-Engineering/functional-programming-signal
- 01-Compound-Engineering/ralph-loop
- 01-Compound-Engineering/agent-reliability-chasm
- 02-Startup-Advice/liquidation-cadence
- 02-Startup-Advice/value-creation

---

## 9. Word Count Target

| Section | Target Words |
|---------|-------------|
| 4.1 Mental Model | 1,200 |
| 4.2 Layer 1 (Claude Code) | 1,800 |
| 4.3 Layer 2 (Repository) | 2,500 |
| 4.4 Layer 3 (Meta-Engineering) | 2,200 |
| 4.5 Layer 4 (Closed-Loop) | 2,800 |
| 4.6 Building the Factory | 2,500 |
| 4.7 MCP Servers | 2,800 |
| 4.8 Complete Example | 1,500 |
| 4.9 Measuring Success | 900 |
| 4.10 Pitfalls | 1,000 |
| 4.11 Maturity Progression | 700 |
| **Total Chapter Content** | **~20,000 words** |

**Formatting Notes**:
- Code blocks: Count as prose equivalent (~1.5x word count due to formatting)
- Diagrams: Reference by number, no word count
- Examples: Inline and substantial
- Exercises: Separate section, not included in chapter word count

**Total Book Impact**:
- Chapter 10 represents ~12% of total book (estimated 170,000 words total)
- Heavy code emphasis: aim for 40% code/examples, 60% prose
- Readability: Every 500 words, include code or diagram

---

## 10. Status: Draft

### Dependencies
- [ ] All source articles finalized and locked
- [ ] Chapter 9 (Context Engineering) completed (prerequisite)
- [ ] Diagram assets created (10 diagrams listed)
- [ ] Code examples tested and validated
- [ ] Exercises peer-reviewed for clarity and difficulty calibration
- [ ] Cross-references verified against final chapter numbering

### Review Checklist
- [ ] Technical accuracy (review with infrastructure SME)
- [ ] Clarity for target audience (junior to mid-level engineers)
- [ ] Code examples compile and run
- [ ] Diagrams are clear and not overloaded
- [ ] Exercises are feasible in stated time window
- [ ] Tone matches rest of book
- [ ] No orphaned sections or broken references

### Timeline
- **Draft completion**: Current
- **Technical review**: Week 2
- **Examples development**: Weeks 2-3
- **Diagrams creation**: Week 3
- **Copy editing**: Week 4
- **Final review**: Week 5
- **Ready for publication**: End of Week 5

### Known Gaps (To Address in Next Draft)
1. More concrete cost calculations for infrastructure investment
2. Handling optimization loop failures and escalation paths
3. Team dynamics when infrastructure reduces manual work
4. Transition strategies for existing codebases
5. Advanced MCP patterns (caching, pagination, subscriptions)

---

## Appendix: Quick Reference

### The Four-Layer Harness at a Glance
```
Layer 4: Closed-Loop Optimization
  └─ Telemetry → Constraints → Agent → Code → Test → Loop
  └─ Purpose: Self-healing infrastructure

Layer 3: Meta-Engineering
  └─ Automation scripts, tests for tests, agent swarms
  └─ Purpose: Process automation

Layer 2: Repository Engineering
  └─ OTEL, tests, DDD, Docker, git
  └─ Purpose: Environmental clarity

Layer 1: Claude Code
  └─ claude.md, hooks, scope, constraints
  └─ Purpose: Raw harness around LLM
```

### MCP Resource Types
| Resource | Purpose | Query Example |
|----------|---------|--------------|
| architecture-graph | Dependencies | `architecture-graph://auth` |
| pattern-examples | Code examples | `pattern-examples://factory-functions` |
| recent-changes | Git history | `recent-changes://last-week` |
| test-coverage | Test metrics | `test-coverage://auth` |
| performance-metrics | Runtime stats | `performance-metrics://api` |
| code-search | AST search | `code-search://authenticate` |

### Success Metrics
- Automation Coverage: >70% of recurring tasks (3 months)
- Time Savings: >20 hours/week (2 months)
- Tool ROI: >5x (1 month after build)
- Infrastructure Leverage: >10x (6 months)
- Tool Reuse: >30% used beyond originator

---

**Document Version**: 0.2
**Last Updated**: January 28, 2026
**Status**: Ready for Technical Review

---

## Appendix: Content Expansion Opportunities (Jan 28, 2026 KB Analysis)

Based on knowledge base scan, the following content could enrich Chapter 13:

### From ci-cd-agent-patterns.md
- **Tiered Verification**: Different gates for PRs (lightweight) vs nightly (comprehensive)
- **Response Caching**: Hash-based caching for fast PR checks, refreshed nightly
- **Cost Budget Enforcement**: Budget limits per trigger type ($2 for PR, $50 for nightly)
- **Behavioral Regression Tests**: Quality metrics (precision, recall) for agent outputs
- **Multi-Model Testing**: Testing agents against different models weekly
- **Canary Deployments**: Gradual rollout with metric-based abort

### From agent-memory-patterns.md
- **Three-Tier Memory Hierarchy**: Session (ephemeral), File-based (TASKS.md, progress.txt), Event-sourced (append-only log)
- **Checkpoint After Every Tool Call**: Ratchet effect for fault tolerance
- **Webhook Integration**: External systems resume agents via POST /webhook/resume/:threadId

### From ai-cost-protection-timeouts.md
- **Five-Layer Protection**: Job timeout, request token caps, input size limits, budget alerts, model selection
- **Cost Calculation Examples**: Real pricing for scheduled code review workflows
- **Model Switching Strategy**: Haiku for 80% of work, Sonnet for complex 20%

### From agent-reliability-chasm.md
- **Exponential Reliability Problem**: 95% per-action → 36% overall for 20-step task
- **Four-Turn Framework**: Understand, Decide, Execute, Verify
- **Pre-Action Checks**: Required info available? Ambiguous request? Prerequisites met?
- **Post-Action Verification**: Check outcomes, not just API responses

### From 12-factor-agents.md
- **Factor 5**: Unify execution state and business state (derive from events)
- **Factor 6**: Launch/Pause/Resume with simple APIs
- **Factor 7**: Contact humans with structured tool calls
- **Factor 10**: Small, focused agents (3-20 steps max)
- **Factor 12**: Agent as stateless reducer (fold over events)
