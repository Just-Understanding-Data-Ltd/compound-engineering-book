# Chapter 7 PRD: Context Engineering Deep Dive
## Managing Context Windows Effectively for AI-Assisted Code Generation

**Status**: Draft | **Target Audience**: Intermediate to Advanced Engineers | **Estimated Reading Time**: 40-50 min

---

## 1. Overview

Context engineering is the discipline of managing finite information capacity—the context window—as an information channel. This chapter treats LLM context windows as constrained resources governed by information theory. By understanding entropy (uncertainty), information content (constraint value), mutual information (context effectiveness), and channel capacity (token limits), engineers can predictably control LLM behavior. The chapter provides mathematical foundations and practical patterns to maximize code generation quality while minimizing token waste, enabling long-running sessions without degradation, and systematically debugging when outputs diverge from expectations.

---

## 2. Learning Objectives

After completing this chapter, readers will be able to:

1. **Quantify uncertainty in LLM behavior** using entropy and understand why quality gates work through exponential entropy reduction
2. **Design high-information-density context** by recognizing that types/tests provide 3-10× more bits/token than comments
3. **Implement progressive disclosure patterns** to load context only when needed, scaling agent capabilities beyond context window limits
4. **Prevent context rot** using auto-compacting strategies, maintaining accuracy in sessions exceeding 100+ messages
5. **Debug AI generation failures systematically** using the hierarchical debugging protocol (Context → Prompting → Model Power → Manual)
6. **Apply context-efficient backpressure** to minimize noise and maintain signal-to-noise ratio in long sessions

---

## 3. Source Articles

- [Information Theory for Coding Agents: Mathematical Foundations of LLM Code Generation](../context-engineering/information-theory-coding-agents.md)
- [Progressive Disclosure: Load Context Only When Needed](../context-engineering/progressive-disclosure-context.md)
- [Context Rot Prevention: Auto-Compacting for Long AI Sessions](../context-engineering/context-rot-auto-compacting.md)
- [Context-Efficient Backpressure](../context-engineering/context-efficient-backpressure.md)
- [Context Debugging Framework: Systematic Problem-Solving for AI Code Generation](../context-engineering/context-debugging-framework.md)

---

## 4. Detailed Outline

### Section 4.1: Information Theory Foundations

#### 4.1.1 The Context Window as an Information Channel
- **Concept**: Context windows are finite-capacity channels transmitting information from human to AI
- **Channel capacity formula**: C = max I(X;Y) - maximum mutual information
- **Real-world limits**: Claude Sonnet at 200K tokens × ~4 bits/token = 800K bits capacity
- **The bottleneck**: Cannot exceed capacity without truncation, dilution, or compression
- **Key insight**: Unlike human memory, LLM context has hard limits requiring deliberate optimization

#### 4.1.2 Entropy: Measuring Uncertainty in Code Generation
- **Definition**: H(X) = -∑ P(x) log₂ P(x) - uncertainty in bits
- **Code generation interpretation**: High entropy = many possible outputs (unpredictable), low entropy = few outputs (predictable)
- **Quality gates as entropy filters**: Each gate exponentially reduces valid programs
  - All syntactically valid programs: H = 20 bits (1M+ programs)
  - After type checker: H = 15 bits (32K programs)
  - After linter: H = 12 bits (4K programs)
  - After tests: H = 5 bits (32 programs)
- **Practical measurement**: Estimate via test failure rate
  - High entropy: 30-50% failure rate
  - Medium entropy: 10-20% failure rate
  - Low entropy: <5% failure rate

#### 4.1.3 Information Content: Quantifying Constraint Value
- **Definition**: I(x) = -log₂ P(x) - how much you learn when event x occurs
- **Constraint hierarchy**:
  - Types: ~3.3 bits per constraint (eliminate 90% of implementations)
  - Tests: ~4.3 bits per constraint (eliminate 95% of implementations)
  - Comments: ~0.15 bits per constraint (eliminate 10% of implementations)
- **Why types > comments**: Types provide 11× more information per token
- **Token efficiency metric**: bits/token ratio
  - High density (types/tests): 3.5-5.0 bits/token
  - Medium density (structured docs): 2.0-3.5 bits/token
  - Low density (prose): 0.5-2.0 bits/token

#### 4.1.4 Mutual Information: Understanding Context Effectiveness
- **Definition**: I(X;Y) = H(X) - H(X|Y) - how much context tells you about output
- **Interpretation**:
  - High MI: Context strongly determines output (effective)
  - Low MI: Context doesn't help (ineffective)
- **Measuring MI via output variance**: Generate same prompt 10 times
  - 1-2 unique outputs: High MI (good context)
  - 3-5 unique outputs: Medium MI (improve context)
  - 6+ unique outputs: Low MI (context needs work)
- **High-MI patterns**:
  - Show, don't tell: Working examples > explanations
  - Be specific: Concrete constraints > vague guidelines
  - Anti-patterns: Show what NOT to do
  - Multiple examples: More examples > single example

#### 4.1.5 Channel Capacity: Optimizing Information Transfer
- **Definition**: Maximum bits that can be reliably transmitted through context window
- **Optimization strategies**:
  1. **Maximize information density**: Use types/tests over verbose documentation
  2. **Hierarchical context loading**: Load only relevant context for task
  3. **Prompt caching**: Cache stable, high-information content
  4. **Context metrics**:
     - Utilization: tokens_used / capacity (target: 60-80%)
     - Information density: bits/token (target: 3.5+)
     - Mutual information: output variance (target: <2 unique outputs)

#### 4.1.6 Integrating All Four Concepts
- **The full pipeline**: Context (channel capacity) → Generation (entropy) → Quality gates (information filtering) → Output (low entropy)
- **Authentication function example**: Concrete walkthrough from context to final implementation
- **Cost-benefit analysis**: Calculate ROI of different quality gates

---

### Section 4.2: Progressive Disclosure Patterns

#### 4.2.1 Three-Level Architecture
- **Level 1: Metadata Layer** (always loaded)
  - Minimal information (~50-100 tokens per skill)
  - Lets agent recognize when skill applies
  - Example: SKILL.md frontmatter with name, description, triggers
- **Level 2: Core Instructions** (loaded when relevant)
  - Complete skill instructions (~500-2000 tokens per skill)
  - Full working knowledge for the task
  - Loaded only when agent determines relevance
- **Level 3+: Supplementary Resources** (loaded as needed)
  - Specialized content for sub-tasks
  - Variable token cost, only loaded explicitly
  - Example: advanced.md, troubleshooting.md referenced from core

#### 4.2.2 Implementation Patterns
- **Skill directory structure**:
  ```
  skills/
  ├── pdf/
  │   ├── SKILL.md          # Level 1 + 2
  │   ├── forms.md          # Level 3
  │   └── reference.md      # Level 3
  ├── git/
  │   ├── SKILL.md
  │   └── workflows.md
  ```
- **Metadata registry**: Load descriptions at startup, full skills on-demand
- **Lazy reference loading**: Skills reference supplementary docs that agent discovers
- **Real-world CLAUDE.md hierarchy**: Root → subdirectory → file-specific context

#### 4.2.3 Integration Strategies
- **Progressive Disclosure + Hierarchical Context**: CLAUDE.md files at each level
- **Progressive Disclosure + Model Switching**: Fast models for selection, capable models for execution
- **Progressive Disclosure + Prompt Caching**: Cache frequently-used skill combinations
- **Agents with filesystem access**: Can read files on-demand without loading into context

#### 4.2.4 Benefits and Metrics
- **Scalability**: Add unlimited skills for <500 tokens metadata overhead
- **Cost efficiency**: 87% savings by loading only relevant context
- **Maintainability**: Update skills independently
- **Unbounded capability**: Agents not limited by context window when using filesystem
- **Success metrics**:
  - Token efficiency: <3000 tokens per task
  - Skill discovery: >95% correct skill loaded
  - Cost reduction: 70% less than baseline

---

### Section 4.3: Context Rot and Auto-Compacting

#### 4.3.1 The Problem: Context Rot in Long Sessions
- **Definition**: Gradual degradation of output quality as stale information accumulates
- **Symptoms**:
  - References outdated code
  - Suggests old architecture
  - Confuses current state
  - Hallucinates about deleted files
  - Decreases accuracy over time
- **Signal-to-noise degradation**:
  - Messages 1-20: 90% signal (mostly relevant)
  - Messages 21-50: 60% signal (mix of current and obsolete)
  - Messages 51-100: 30% signal (stale context dominant)
  - Messages 100+: 10% signal (buried in history)
- **Real-world cost**: 60-message gap between architecture change and AI awareness

#### 4.3.2 Auto-Compacting Mechanisms
- **Claude Code's built-in approach**:
  - Monitors context size and token usage
  - Triggers when context grows large (~100K tokens)
  - Summarizes completed work
  - Preserves key decisions and current state
  - Removes intermediate debugging steps
- **Result**: Context shrinks 70-90% while retaining critical information
- **Before/After**: 150 messages at 100K tokens → 10 messages at 15K tokens

#### 4.3.3 Manual Compacting Strategies
- **Task-driven compacting**: Summarize every 5-10 completed tasks
- **Feature-driven compacting**: Compact entire feature before starting next
- **Recursive compacting** (multi-level):
  - Level 1 (tasks): 1-2 sentences per completed task
  - Level 2 (features): Paragraph per completed feature
  - Level 3 (sprints): DIGEST.md per milestone
  - Level 4 (versions): Archive to CHANGELOG.md
- **Compacting prompt patterns**:
  - Simple summary: Completed work, decisions, current state, pending
  - Structured summary: Sections for completed/decisions/pending/tests
  - Migration summary: Before/after/steps/migration status

#### 4.3.4 Real-World Compacting Cycle
- **Before compacting**:
  - 150+ messages
  - 100K tokens
  - References to deleted code
  - 60% generation accuracy
- **Trigger compacting**
- **After compacting**:
  - ~10 messages
  - ~15K tokens
  - Clear current state
  - 95% generation accuracy

#### 4.3.5 Integration with Other Patterns
- **Combine with DIGEST.md**: Store compacted summaries at package level
- **Combine with todo lists**: Task structure becomes compacting boundaries
- **Combine with hierarchical CLAUDE.md**: Update domain files with compacted learnings
- **Combine with git boundaries**: Compact at PR/commit boundaries
- **Automatic triggers**:
  - After completing 5-10 tasks
  - After finishing a feature
  - When switching contexts
  - When AI references deleted code
  - After 100+ messages
  - End of day

#### 4.3.6 Best Practices
- **Compact proactively**: After features, not when rot is obvious
- **Use task lists as structure**: Clear boundaries for compacting
- **Preserve decisions, remove noise**: Keep architecture, delete debugging
- **Set regular intervals**: Every 80-100 messages or per feature
- **Transfer knowledge**: Compacted summaries perfect for documentation
- **Metrics**: Context reduction 80-90%, accuracy improvement 45%+

---

### Section 4.4: Context-Efficient Backpressure

#### 4.4.1 The Problem: Context Wasted on Verbose Output
- **Issue**: Successful test/build/lint runs generate 200+ lines of output
- **Context cost**: 2-3% of window conveying information needing <10 tokens
- **Model performance zone**: Claude optimizes at ~75K tokens; beyond is "dumb zone"
- **Cost of dumb zone**: Human management 10× more expensive than token savings

#### 4.4.2 The `run_silent()` Pattern
- **Core idea**: Swallow output on success, dump on failure
- **Bash implementation**:
  ```bash
  run_silent() {
      local description="$1"
      local command="$2"
      local tmp_file=$(mktemp)

      if eval "$command" > "$tmp_file" 2>&1; then
          printf "  ✓ %s\n" "$description"
          rm -f "$tmp_file"
          return 0
      else
          local exit_code=$?
          printf "  ✗ %s\n" "$description"
          cat "$tmp_file"
          rm -f "$tmp_file"
          return $exit_code
      fi
  }
  ```
- **Usage**: Chain multiple test suites with readable output
- **Output**: ✓/✗ indicators with full details on failure only

#### 4.4.3 Progressive Refinement Techniques
- **Enable failFast**: Process one failure at a time (pytest -x, jest --bail)
- **Filter irrelevant output**: Strip node_modules references, timing info
- **Framework-specific parsing**: Extract only failure summaries
- **Anti-patterns to avoid**:
  - Output swallowing then describing (wastes more tokens)
  - Piping to head/tail (forces re-runs if error beyond limit)

#### 4.4.4 TypeScript/JavaScript Implementation
- **Programmatic `runSilent()`**: Spawn-based execution with conditional logging
- **Agent tooling integration**: Return success indicator or full failure output
- **Full test runner example**: Complete implementation with type check, lint, unit, integration tests

#### 4.4.5 Key Principle
- **Deterministic output beats non-deterministic parsing**: If you know what matters, don't make model decide
- **Results**: Keeps context in "smart zone" while processing comprehensive test suites

---

### Section 4.5: Systematic Context Debugging Framework

#### 4.5.1 The Hierarchical Debugging Protocol
- **Core principle**: Debug in order of likelihood of success, not convenience
- **Four-layer hierarchy with success rates**:
  1. **Context Layer** (60% of issues): Missing information, files, examples, architecture
  2. **Prompting Layer** (25% of issues): Vague instructions, missing examples, unclear constraints
  3. **Model Power** (10% of issues): Task exceeds model's reasoning capability
  4. **Manual Override** (5% of issues): Requires human judgment, domain expertise, creativity

#### 4.5.2 Layer 1: Context (60% of Issues)
- **Problem signature**: Plausible but incorrect code not fitting codebase
- **Debugging checklist**:
  1. Include relevant code files showing existing patterns
  2. Provide system architecture and design decisions
  3. Include error messages and stack traces
  4. Show database schemas and API contracts
  5. Provide examples of expected behavior
- **Efficiency gains**: Systematic context addition resolves 90% of issues in 5 minutes vs. 50 minutes trial-and-error
- **How to gather context**: Use `@` mentions, reference CLAUDE.md files, include relevant tests
- **Success criteria**: All relevant context included and AI still produces incorrect output

#### 4.5.3 Layer 2: Prompting (25% of Issues)
- **Problem signature**: AI has context but output doesn't meet requirements
- **Debugging checklist**:
  1. Add specific examples of desired output
  2. Include edge cases and constraints
  3. Provide clear success criteria
  4. Break complex tasks into steps
  5. Use structured formats (tables, JSON, code blocks)
- **Common improvements**:
  - Before: "Format user data for display"
  - After: Show input/output examples with exact expected format
- **Structured prompting**: Function signature, validation rules, return type, examples
- **Success criteria**: Specific examples provided and AI still produces incorrect output

#### 4.5.4 Layer 3: Model Power (10% of Issues)
- **Problem signature**: Context and prompting exhausted but output fails
- **Root cause**: Task genuinely exceeds model's reasoning capability
- **When to escalate**: Only when Layers 1-2 complete and task requires advanced reasoning
- **Model escalation strategy**: Standard → Powerful → Specialized → Manual
  - Claude Sonnet for most tasks (low cost, capable)
  - Claude Opus for complex reasoning ($0.075 vs. $0.015 worth it)
  - Recognize when specialized models help
- **Cost-benefit**: Use cheaper model first with good context, escalate only if needed
- **Example**: Real-time collaborative editing needs deeper reasoning than API endpoint

#### 4.5.5 Layer 4: Manual Override (5% of Issues)
- **Problem signature**: All AI layers fail regardless of context/capability
- **Scenarios requiring manual**:
  - Deep domain expertise (medical diagnosis)
  - Human intuition/creativity (brand identity)
  - Ambiguous/contradictory requirements
  - Legacy systems with tribal knowledge
- **Hybrid approach**: Human solves core problem, AI implements/scales solution
- **Success criteria**: All 3 previous layers exhausted
- **Integration**: Document manual solutions in CLAUDE.md for future context

#### 4.5.6 Practical Application
- **Real-world example**: API endpoint failure with null reference bug
  - Layer 1: Add error logs and existing patterns → Fixed
  - Alternative: Layer 2 with edge cases and constraints → Fixed
  - Alternative: Layer 3 with Opus for complex logic → Fixed
  - Alternative: Layer 4 when no AI can handle → Hybrid approach
- **Time comparison**: Unsystematic 50 min vs. systematic 5 min with proper ordering

#### 4.5.7 Best Practices
1. Always start with Layer 1 (context)
2. Don't waste tokens on genuinely hard problems
3. Build context libraries (CLAUDE.md)
4. Document solutions for future context
5. Track success rate by layer
- **Expected distribution**: Context 60%, Prompting 25%, Model 10%, Manual 5%

---

### Section 4.6: Synthesis and Application

#### 4.6.1 Information Theory Meets Practice
- **How concepts work together**:
  - Context (channel capacity) limits what can be provided
  - Generation produces distribution with entropy H(X)
  - Quality gates filter by providing information content
  - Output entropy drops to near-zero
- **All while staying within channel limits**

#### 4.6.2 Long-Session Architecture
- **Progressive disclosure**: Scale capabilities beyond context window
- **Auto-compacting**: Maintain freshness in 100+ message sessions
- **Backpressure**: Keep signal-to-noise high for accuracy
- **Combined**: Agents that stay productive indefinitely

#### 4.6.3 Cost Optimization
- **Context density prioritization**: types/tests > examples > documentation > prose
- **Token budgeting**: Allocate tokens by information density
- **Caching strategy**: Cache stable, high-MI content
- **Model selection**: Sonnet with good context before Opus

#### 4.6.4 Quality Gates as Investment
- **ROI analysis**: Each gate reduces entropy exponentially
- **Setup cost vs. entropy reduction**: TypeScript highest ROI
- **Compound effect**: Gates stack multiplicatively not additively

---

## 5. Key Examples

### 5.1 Authentication Function Deep Dive
- **Step 1**: Show context (types, existing patterns, tests)
- **Step 2**: Trace entropy reduction through generation
- **Step 3**: Demonstrate quality gates filtering
- **Step 4**: Final low-entropy implementation

### 5.2 Long-Session Compacting Cycle
- **Initial state**: 150 messages, context rot evident
- **Trigger**: AI references deleted code
- **Compacting process**: Summarize work, preserve decisions, remove noise
- **Result**: 10 messages, fresh context, improved accuracy

### 5.3 Progressive Disclosure Skill Structure
- **Metadata registry**: List of skills with triggers
- **Core instruction loading**: When agent recognizes need
- **Lazy supplementary loading**: When specialized case arises
- **Result**: Unbounded capabilities within context limits

### 5.4 Debugging Failure Case
- **Problem**: AI generates wrong endpoint
- **Layer 1 debugging**: Add existing patterns and error logs
- **Result**: AI sees issue immediately
- **Time savings**: 45 minutes vs. 5 minutes

### 5.5 Context-Efficient Test Runner
- **Before**: 200+ lines of test output consuming 500+ tokens
- **After**: "✓ Auth tests" consuming 10 tokens
- **Failure case**: Only dump when something fails
- **Result**: Stay in optimal 75K token zone

### 5.6 Information Density Prioritization
- **Context budget allocation**: Sort by bits/token
- **Include first**: Types (4 bits/token), Tests (3.5 bits/token)
- **Include last**: Documentation (1 bit/token)
- **Result**: Maximum information within capacity

---

## 6. Diagrams Needed

### 6.1 Information Theory Pipeline
- **Visual**: Flow chart showing Context → Entropy → Quality Gates → Output
- **Description**: How the four information theory concepts integrate in code generation
- **Components**: Channel capacity limiting input, entropy in generation, gates filtering outputs, final low entropy

### 6.2 Entropy Reduction Through Quality Gates
- **Visual**: Bar chart or pyramid showing entropy dropping at each gate
- **Description**: Exponential reduction from syntactically valid to type-safe to tested code
- **Data**: H values at each stage (20 bits → 15 → 12 → 5)

### 6.3 Progressive Disclosure Three-Level Architecture
- **Visual**: Pyramid or nested boxes showing Level 1 (metadata), Level 2 (core), Level 3 (supplementary)
- **Description**: How context loads on-demand
- **Metrics**: Token cost at each level, when each loads

### 6.4 Context Rot Signal-to-Noise Degradation
- **Visual**: Line chart showing signal-to-noise ratio declining through message count
- **Description**: How irrelevant context accumulates
- **X-axis**: Message number (0-150)
- **Y-axis**: Signal-to-noise percentage (100% → 10%)

### 6.5 Hierarchical Debugging Protocol
- **Visual**: Pyramid with four layers: Context (60%) → Prompting (25%) → Model (10%) → Manual (5%)
- **Description**: Probability of issue at each layer
- **Emphasis**: Starting point, success rate, when to escalate

### 6.6 Run Silent Output Comparison
- **Visual**: Two-column before/after showing verbose vs. compressed test output
- **Description**: Token savings from backpressure pattern
- **Data**: Tokens used, readability, success indicator vs. full output

### 6.7 Multi-Level Recursive Compacting
- **Visual**: Nested structure showing compression at each level
- **Description**: How compacting works at task/feature/sprint/version granularity
- **Result**: Exponential context reduction

### 6.8 Channel Capacity Utilization Dashboard
- **Visual**: Gauge/meter showing tokens used vs. capacity
- **Description**: What good/bad utilization looks like
- **Components**: Total capacity, used, utilization %, information density

---

## 7. Exercises

### 7.1 Exercise: Measure Entropy in Your Codebase
**Objective**: Apply entropy concept to real code

**Instructions**:
1. Pick a function you want to generate (e.g., user validation)
2. Generate it 10 times with same prompt, no context
3. Count unique implementations produced
4. Estimate entropy: H ≈ log₂(unique_outputs)
5. Add one constraint (type signature)
6. Repeat 10 times, count unique outputs
7. Measure entropy reduction
8. Add another constraint (test cases)
9. Repeat and measure again
10. Document how entropy dropped with each constraint

**Expected outcome**: Demonstrate exponential entropy reduction, understand information content

**Deliverable**: Chart showing entropy values at each stage, brief analysis

---

### 7.2 Exercise: Design Progressive Disclosure Skill
**Objective**: Architect a skill using three-level progressive disclosure

**Instructions**:
1. Choose a domain (PDF processing, git operations, testing, etc.)
2. **Level 1**: Create metadata (~50 tokens)
   - Name, description, triggers
   - Include in registry
3. **Level 2**: Write core instructions (~1000 tokens)
   - Core capabilities
   - Common patterns
   - References to supplementary docs
4. **Level 3**: Create supplementary docs (variable)
   - Advanced patterns
   - Edge cases
   - Troubleshooting
5. **Test loading**: Simulate what agent loads at each stage
6. **Measure efficiency**: Compare to flat structure (all at once)

**Expected outcome**: Understand progressive disclosure mechanics, design scalable skills

**Deliverable**: Folder structure with files, token count at each level, loading simulation

---

### 7.3 Exercise: Debug Using Hierarchical Protocol
**Objective**: Practice systematic debugging through four layers

**Instructions**:
1. Start with AI generating incorrect code (pick a real failure case)
2. **Layer 1 - Context**: Add relevant files, architecture, error logs
   - Does context fix it? If yes, note time and stop
   - If no, proceed to Layer 2
3. **Layer 2 - Prompting**: Add specific examples, edge cases, success criteria
   - Does prompting fix it? If yes, note time and stop
   - If no, proceed to Layer 3
4. **Layer 3 - Model Power**: Escalate to more powerful model (if available)
   - Does capability fix it? If yes, note time and stop
   - If no, proceed to Layer 4
5. **Layer 4 - Manual**: Manually implement core logic, have AI scale/expand
   - Does hybrid approach work? Note time and stop
6. **Analysis**:
   - Which layer fixed the issue?
   - Time spent at each layer
   - Total time vs. trial-and-error approach

**Expected outcome**: Internalize debugging hierarchy, recognize quick wins at context layer

**Deliverable**: Annotated debugging session showing layer transitions, time measurements, root cause

---

## 8. Cross-References

### Within Compound Engineering Book
- **Chapter 2: Signal and Noise**: Foundation for understanding context as signal/noise ratio
- **Chapter 3: Quality Gates**: How gates implement entropy reduction from information theory
- **Chapter 5: Hierarchical Context Patterns**: Practical implementation of progressive disclosure
- **Chapter 6: Long-Session Stability**: Prerequisites for understanding context rot
- **Chapter 8: Cost Optimization**: How information theory informs token budgeting
- **Chapter 10: Advanced Architectures**: Multi-agent systems using progressive disclosure

### External References
- Shannon, C. (1948). "A Mathematical Theory of Communication" - Original information theory foundations
- MIT OpenCourseWare 6.441 (Information Theory) - Comprehensive course materials
- Anthropic Engineering Blog: "Equipping Agents with Skills" - Progressive disclosure in practice
- HumanLayer Blog: "Context-Efficient Backpressure" - Backpressure pattern origin

### Related Knowledge Base Articles
- [Entropy in Code Generation](../compound-engineering/entropy-in-code-generation.md)
- [Quality Gates as Information Filters](../compound-engineering/quality-gates-as-information-filters.md)
- [Hierarchical Context Patterns](../compound-engineering/hierarchical-context-patterns.md)
- [Prompt Caching Strategy](../compound-engineering/prompt-caching-strategy.md)
- [Clean Slate Trajectory Recovery](../compound-engineering/clean-slate-trajectory-recovery.md)

---

## 9. Word Count Target

- **Chapter Total**: 8,000-10,000 words
- **Section Breakdown**:
  - Information Theory Foundations: 2,000-2,500 words (40% of chapter)
  - Progressive Disclosure: 1,500-1,800 words (18% of chapter)
  - Context Rot and Auto-Compacting: 1,500-1,800 words (18% of chapter)
  - Context-Efficient Backpressure: 800-1,000 words (10% of chapter)
  - Systematic Debugging Framework: 1,500-2,000 words (18% of chapter)
  - Synthesis and Application: 500-700 words (7% of chapter)

- **Supporting Elements**:
  - Code examples: ~1,000 words (inline, not counted separately)
  - Diagrams: 8 diagrams with 50-100 word descriptions each
  - Exercises: 3 exercises with 150-200 word instructions each
  - Best practices boxes: ~500 words distributed throughout

---

## 10. Key Learning Path

**Session 1 (Information Theory - 60 min)**:
- Read Section 4.1 (Foundations)
- Complete Exercise 7.1 (Measure Entropy)
- Study Diagrams 6.1, 6.2

**Session 2 (Progressive Disclosure - 45 min)**:
- Read Section 4.2 (Progressive Disclosure)
- Complete Exercise 7.2 (Design Skill)
- Study Diagram 6.3

**Session 3 (Long Sessions - 50 min)**:
- Read Section 4.3 (Context Rot)
- Study Diagram 6.4
- Practice compacting with task list

**Session 4 (Debugging - 45 min)**:
- Read Sections 4.4-4.5 (Backpressure + Debugging)
- Complete Exercise 7.3 (Debug Session)
- Study Diagrams 6.5, 6.6

**Session 5 (Synthesis - 40 min)**:
- Read Section 4.6 (Synthesis)
- Review all diagrams
- Plan how to apply in current projects

---

## 11. Assessment Strategy

### Knowledge Checks
- Explain entropy, information content, mutual information, channel capacity concepts
- Draw the information theory pipeline
- Describe three-level progressive disclosure
- Explain context rot symptoms and prevention
- State the debugging hierarchy and why it's ordered that way

### Practical Competency
- Design a progressive disclosure skill structure
- Implement `run_silent()` in own codebase
- Measure entropy reduction with quality gates
- Debug real AI failure through four-layer protocol
- Implement auto-compacting in long sessions

### Integration Assessment
- Apply information theory to prioritize context for task
- Combine progressive disclosure + auto-compacting in agent design
- Measure and optimize channel capacity utilization
- Track debugging metrics for own team

---

## 12. Status

**Draft** - Ready for content development

**Next Steps**:
1. Expand each section with detailed content from source articles
2. Create all 8 diagrams with annotations
3. Write code examples and integrate inline
4. Develop all three exercises with solutions
5. Create best-practices boxes and callouts
6. Add inline citations to source articles
7. Create instructor guide with timing suggestions
8. Develop quiz/assessment questions
9. Record video walkthroughs for exercises
10. Gather feedback from beta readers

---

## 13. Chapter Dependencies and Sequencing

**Prerequisite Chapters**:
- Chapter 1: Fundamentals (what is code generation, LLM basics)
- Chapter 2: Signal and Noise (context as signal/noise)

**This Chapter Enables**:
- Chapter 8: Cost Optimization (information theory for token budgeting)
- Chapter 9: Production Patterns (long-session stability)
- Chapter 10: Multi-Agent Systems (progressive disclosure at scale)

---

## 14. Estimated Development Time

| Component | Time | Owner |
|-----------|------|-------|
| Content development | 40 hours | SME |
| Diagram creation | 8 hours | Designer |
| Code examples | 12 hours | Engineer |
| Exercise solutions | 10 hours | Engineer |
| Internal review | 8 hours | Reviewers |
| Beta testing | 15 hours | Readers |
| **Total** | **93 hours** | |

---

## 15. Success Criteria

**Chapter is successful when**:

- [ ] Readers can explain information theory concepts (entropy, MI, channel capacity) with examples
- [ ] Readers can measure entropy reduction in their own code
- [ ] Readers can design and implement progressive disclosure patterns
- [ ] Readers can implement auto-compacting in long sessions
- [ ] Readers can systematically debug AI failures (60% at context layer)
- [ ] Readers can reduce context size by 70-90% while improving accuracy
- [ ] Readers report 45% improvement in AI suggestion quality
- [ ] Readers complete all three exercises and apply learnings to projects
- [ ] Reader satisfaction score: >8/10
- [ ] Zero technical errors or misconceptions in content

---

**Document created**: 2026-01-26
**Last updated**: 2026-01-26
**Version**: 1.0 Draft
