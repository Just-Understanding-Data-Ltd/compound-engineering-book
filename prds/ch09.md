# Chapter 11 PRD: Sub-Agent Architecture
**Delegating to Specialized Sub-Agents**

> **Note:** This PRD file is named `ch09.md` for historical reasons (original chapter numbering before the 15-chapter restructure). The content corresponds to actual Chapter 11: Sub-Agent Architecture.

## Status: Draft
**Last Updated:** January 28, 2026
**Author:** Claude Code (with James Phoenix source material)

---

## 1. Overview

Chapter 9 covers the shift from monolithic single-agent workflows to **specialized sub-agent architectures**, a pattern that mirrors real development teams. Rather than asking one generalist agent to handle backend, frontend, testing, and code review, we delegate to specialized agents with focused expertise, restricted tool access, and behavioral flows that enforce patterns. The chapter demonstrates that this division of labor produces substantially higher-quality outputs at the cost of orchestration complexity and initial latency, but ultimately reduces total time-to-production by eliminating revision cycles. Readers learn when sub-agents are justified (large codebases, full-stack features, production-critical code) and how to implement them using a three-layer context hierarchy, specialized prompts, and orchestration patterns. The chapter also covers complementary patterns: swarm-based analysis for thoroughness, actor-critic loops for pre-deployment quality gates, and parallel agents for monorepo-wide changes.

---

## 2. Learning Objectives

After completing this chapter, readers will be able to:

- **Identify the generalist trap** and recognize when single-agent quality degrades on large codebases
- **Design specialized sub-agent teams** with distinct roles (backend engineer, frontend engineer, QA engineer, code reviewer) and tailored behavioral flows
- **Implement the three-layer context hierarchy** (root patterns, agent-specific flows, package-specific conventions) to maintain consistency across sub-agents
- **Apply tool access control** to constrain agent capabilities and prevent agents from reaching invalid states (e.g., read-only code reviewers that can't introduce new bugs)
- **Execute accuracy vs. latency trade-offs**: knowing when sub-agents justify slower response times for higher quality
- **Deploy swarm patterns** to multiply perspectives (security + performance + edge cases) or confidence (same analysis 4x) for thorough pre-deployment review
- **Build actor-critic loops** that use an adversarial critic to catch vulnerabilities and architectural violations before human review, reducing review cycles by 50%+
- **Orchestrate parallel agents across monorepos** to update 20+ packages in 15 minutes instead of 2+ hours while maintaining consistency
- **Measure success** with metrics: issue density per round, human review cycles saved, time-to-production, and code quality scores

---

## 3. Source Articles

### Primary Sources
- `/01-Compound-Engineering/context-engineering/sub-agent-architecture.md` – Full architecture guide with three-layer context patterns and orchestration
- `/01-Compound-Engineering/context-engineering/sub-agents-accuracy-vs-latency.md` – Trade-off analysis and decision framework
- `/01-Compound-Engineering/context-engineering/agent-swarm-patterns-for-thoroughness.md` – Multi-perspective and multi-run swarm strategies
- `/01-Compound-Engineering/context-engineering/actor-critic-adversarial-coding.md` – Self-review loops and critique dimensions
- `/01-Compound-Engineering/context-engineering/parallel-agents-for-monorepos.md` – Scaling changes across 20+ packages in parallel

### Supplementary Sources (Validated from KB Scan Jan 28, 2026)
- `/01-Compound-Engineering/context-engineering/sub-agent-context-hierarchy.md` – Deep dive into context isolation patterns between sub-agents (3-layer hierarchy: Root, Agent, Package)
- `/01-Compound-Engineering/context-engineering/tool-access-control.md` – Permission boundaries and capability restriction patterns (role-based tool whitelists, bash restrictions, read-only reviewers)
- `/01-Compound-Engineering/context-engineering/orchestration-patterns.md` – Coordinator, Swarm, and Pipeline orchestration implementations with combining patterns
- `/01-Compound-Engineering/context-engineering/actor-critic-pattern.md` – Writer agent + reviewer agent implementation patterns (single agent vs dual agent approaches)
- `/01-Compound-Engineering/context-engineering/agent-memory-patterns.md` – Checkpoint/resume and state persistence for long-running agents (3-tier memory: session, file-based, event-sourced)
- `/01-Compound-Engineering/context-engineering/agent-sdk-patterns.md` – Using Agent SDK v2-preview for sub-agent coordination
- `/01-Compound-Engineering/context-engineering/hierarchical-context-patterns.md` – CLAUDE.md files at every level for context inheritance

---

## 4. Detailed Outline

### 4.1 The Generalist Trap (800 words)

**Objectives**: Establish the problem that sub-agents solve

- **Scenario**: A generalist agent implementing "Add user authentication"
  - Backend: Generic implementation (missing domain patterns, hardcoded tokens, plain text passwords)
  - Frontend: Surface-level React (missing validation, error handling, accessibility)
  - Tests: Superficial (only happy path, no edge cases)
  - Review: Misses security flaws and pattern violations
- **Real-world impact**: For 50K+ LOC codebases
  - Generalist quality: 6/10 backend, 5/10 frontend, 40% tests, 3/10 review effectiveness
  - Specialist quality: 9/10 backend, 8/10 frontend, 85% tests, 8/10 review effectiveness
  - Cost: Higher quality at the cost of orchestration complexity and latency
- **Root causes**:
  - Context switching loses focus
  - Generic patterns instead of domain-specific best practices
  - Shallow expertise in each domain
  - Jack-of-all-trades, master of none
- **When this matters**: Production codebases with established patterns, teams, and quality bars

### 4.2 The Sub-Agent Architecture Solution (1,200 words)

**Objectives**: Introduce the specialized team concept and its benefits

#### 4.2.1 Team Structure (300 words)
- The orchestrator agent (main agent, e.g., "Augster")
  - Understands task requirements
  - Distributes work to specialized agents
  - Coordinates handoffs between agents
  - Aggregates results
- Four core specialist roles:
  - **Backend Engineer**: API endpoints, business logic, DB schemas, validation
  - **Frontend Engineer**: Components, state management, styling, routing
  - **QA Engineer**: Test cases, edge cases, integration tests, E2E tests
  - **Code Reviewer**: Quality check, pattern matching, security scan (read-only)
- Specialization benefits:
  - Focused context per agent
  - Role-specific behavioral flows
  - Restricted tool access per role
  - Clear handoff points

#### 4.2.2 The Three-Layer Context Hierarchy (400 words)
- **Layer 1: Root CLAUDE.md** (Shared Patterns)
  - Architecture (monorepo, layered, dependency rules)
  - TypeScript standards (strict mode, no `any`, explicit types)
  - Error handling (Result pattern, no exceptions in business logic)
  - Testing (integration > unit, AAA pattern)
  - Naming conventions (kebab-case files, camelCase functions, PascalCase classes)
- **Layer 2: Agent Behavioral Flows** (.claude/agents/agent.md, 100-200 lines)
  - When implementing an API endpoint: workflow steps (understand requirements → design endpoint → implement layers → add validation → handle errors → hand off to QA)
  - When creating a component: workflow steps (check library → review design system → implement → create story → hand off to QA)
  - When writing tests: workflow steps (understand feature → write happy path → add edge cases → test error scenarios → verify coverage)
  - What agents DON'T do (prevents scope creep)
  - Tools available to this agent
- **Layer 3: Package-Specific Context** (packages/*/CLAUDE.md)
  - Package-specific patterns and conventions
  - Examples: API route structure, authentication approach, validation schemas
  - Enables agents to match existing style exactly

#### 4.2.3 Tool Access Control (300 words)
- Principle: Restrict capabilities based on role
  - Backend Engineer: Read, Write, Edit, Bash (package-restricted)
  - Frontend Engineer: Read, Write, Edit (UI-only)
  - QA Engineer: Read, Write, Edit, Bash (tests-only)
  - Code Reviewer: Read, Grep, Glob (read-only, prevents buggy "fixes")
- Why read-only reviewers matter:
  - Can identify issues without introducing new bugs
  - Forces fixes through proper channels
  - Prevents reviewer from "helpfully" refactoring
- Implementation (TypeScript example showing permission model)

#### 4.2.4 Sequential vs Parallel Execution (300 words)
- Sequential: Backend → Frontend → QA → Review
  - Use when tasks have dependencies (frontend needs API spec first)
  - Total time: sum of all agent times plus coordination overhead
- Parallel: Backend + Frontend simultaneously
  - Use when tasks are independent
  - Total time: max(concurrent tasks) plus coordination
- Mixed patterns:
  - Parallel Backend and Frontend (independent)
  - Sequential: (Backend + Frontend) → QA → Review
  - Total time: ~15 minutes instead of ~25 minutes

### 4.3 Real-World Example: Payment Feature (1,500 words)

**Objectives**: Walk through a complete sub-agent workflow end-to-end

#### 4.3.1 Step 1: Backend Engineer Implements API (400 words)
- Delegated task: "Implement Stripe payment API endpoint"
- Context layers used: Root CLAUDE.md + backend-engineer.md + packages/api/CLAUDE.md
- Work performed:
  - Design endpoint (POST /api/payments with schema)
  - Implement route layer (Express routing, validation middleware)
  - Implement handler layer (request/response logic)
  - Implement service layer (business logic, Stripe integration, error handling)
  - Use Result pattern for errors
  - No exceptions in business logic
- Output: "Backend complete. Endpoint: POST /api/payments. Schema defined. Ready for frontend."
- Code snippet: Payment schema + handler + service with proper error handling

#### 4.3.2 Step 2: Frontend Engineer Creates UI (400 words)
- Delegated task: "Create payment form component that calls POST /api/payments"
- Context: Backend result (endpoint details) + Root CLAUDE.md + frontend-engineer.md + packages/ui/CLAUDE.md
- Work performed:
  - Check component library for patterns
  - Review design system tokens
  - Implement PaymentForm component with Stripe Elements
  - Add error handling and loading states
  - Match project styling
- Code snippet: PaymentForm component with proper validation, error handling, accessibility
- Output: "Frontend complete. Component: PaymentForm. Ready for testing."

#### 4.3.3 Step 3: QA Engineer Writes Tests (400 words)
- Delegated task: "Write comprehensive tests for payment flow"
- Context: Backend + Frontend results + Root CLAUDE.md + qa-engineer.md
- Work performed:
  - Happy path: successful payment
  - Edge cases: invalid amount, missing fields
  - Error scenarios: auth failure, Stripe API errors
  - Integration: backend + frontend together
  - Coverage: 95%+
- Code snippet: Integration test with mocked Stripe, proper assertions
- Output: "Tests complete. Coverage: 95%. All tests passing. Ready for review."

#### 4.3.4 Step 4: Code Reviewer Audits (300 words)
- Delegated task: "Review payment implementation for security and quality"
- Context: All implementation files (read-only) + Root CLAUDE.md + code-reviewer.md
- Work performed:
  - Security: Stripe secret from env, auth required, input validation, no secrets logged, HTTPS enforced
  - Code Quality: Result pattern, strict TypeScript, naming conventions, separation of concerns
  - Testing: Coverage, edge cases, error scenarios
  - Architecture: Layer boundaries, no violations
- Issues found (severity):
  - Critical: Missing rate limiting on payment endpoint
  - Low: Frontend error messages could be friendlier
- Output: "Review complete. Status: APPROVED WITH MINOR CHANGES. 2 issues found (1 medium, 1 low)."
- Code snippet: Rate limiting middleware recommendation

#### 4.3.5 Step 5: Orchestrator Aggregates Results (200 words)
- Summary document showing all work completed
- Status: READY FOR MERGE after addressing rate limiting
- Action items:
  1. Add rate limiting middleware (Backend Engineer)
  2. Improve error messages (Frontend Engineer)
- This entire workflow completed in ~15 minutes, with higher quality than 40+ minutes of generalist work + 3 revision cycles

### 4.4 Accuracy vs Latency Trade-Offs (900 words)

**Objectives**: Help readers make informed decisions about when sub-agents are worth the cost

#### 4.4.1 Why Sub-Agents Are More Accurate (400 words)
- Fresh context window
  - Main conversation accumulates 50K tokens of noise (previous sessions, abandoned approaches)
  - Sub-agent starts clean with 5K tokens of relevant context
  - Clean context improves reasoning quality
- Specialized system prompts
  - Single agent can't hold expert knowledge for all domains
  - Sub-agent prompt can be 200-300 lines of domain expertise (e.g., security expert checklist)
  - Detailed instructions reduce decision paralysis
- Tool restriction focuses agents
  - Fewer tools = less decision paralysis = better execution
  - Example: Code reviewer has only Read/Grep/Glob (no Edit) = can't introduce bugs

#### 4.4.2 When Sub-Agents Win (300 words)
- **High-stakes decisions** (security review pre-deploy): accuracy > speed
- **Complex analysis** (entire codebase for perf issues): clean context needed
- **Specialized domains** (database optimization): domain expertise needed
- **Production-critical code**: quality matters more than velocity
- **Large codebases** (50K+ LOC): generalist loses coherence

#### 4.4.3 When Main Agent Wins (200 words)
- Quick iterations (fix typo)
- Context already loaded (continue previous task)
- Simple tasks (run the tests)
- Prototypes and experiments
- Time-sensitive hot fixes

#### 4.4.4 The Decision Framework (200 words)
```
Is the task simple and context is fresh?
    YES → Main agent
    NO  ↓
Is it a repeated workflow?
    YES → Script (deterministic)
    NO  ↓
Does it need specialized expertise?
    YES → Custom sub-agent
    NO  ↓
Does it need clean context for complex reasoning?
    YES → Sub-agent
    NO  → Main agent
```

#### 4.4.5 Cost Analysis (200 words)
- Latency cost: Sub-agents add 10-30 seconds (cold start + context gathering)
- But... they reduce revision cycles
  - Without sub-agents: 3-4 review cycles × 30 min/cycle = 90-120 min total
  - With sub-agents: 1-2 review cycles × 30 min/cycle = 30-60 min total
  - Net savings: 30-60 minutes despite higher initial latency
- Token cost: ~5 calls per feature × $0.10-0.30 per call = $0.50-1.50
- Still 50-100x cheaper than human pre-review

### 4.5 Agent Swarm Patterns for Thoroughness (1,000 words)

**Objectives**: Teach readers to multiply perspectives and confidence through parallel swarms

#### 4.5.1 Core Idea (200 words)
- Single agent = one perspective, probabilistic misses
- Multiple agents from different angles = catch what individuals miss
- Formula: 10 agents × 4 runs = 40 perspectives → De-dupe → Solid plan

#### 4.5.2 Pattern 1: Many Perspectives (300 words)
- Run 5-10 agents with different focuses on same code:
  1. Security vulnerabilities
  2. Performance bottlenecks
  3. Code maintainability
  4. Edge cases and error handling
  5. Integration points
  6. Type safety issues
  7. Test coverage gaps
  8. Memory/resource leaks
  9. Race conditions
  10. Logging/observability

- Aggregate findings
- De-duplicate (same issue from multiple agents = higher confidence)
- Rank by severity and frequency
- Example: Swarm on auth module finds SQL injection, missing refresh tokens, no rate limiting simultaneously

#### 4.5.3 Pattern 2: Same Perspective Multiple Times (200 words)
- Run same analysis 4 times to catch probabilistic misses
- LLMs are probabilistic. Four runs find more issues than one run
- Use `Promise.all()` for parallel execution
- De-dupe by issue type
- Confidence score: found by 3/4 runs > found by 2/4 runs > found by 1/4 runs

#### 4.5.4 Pattern 3: Many-Many Perspectives (300 words)
- Maximum thoroughness: 10 perspectives × 4 runs = 40 total analyses
- Use for:
  - Pre-deployment safety checks
  - Security audits
  - Major releases
  - Critical bug investigations
- Implementation:
  - Flatten tasks: [security-1, security-2, performance-1, performance-2, ...]
  - Launch in batches (don't overwhelm)
  - Aggregate all findings
  - Rank by severity + confidence
  - Create prioritized action plan

#### 4.5.5 Specialized Swarm Applications (300 words)

**Spec Drift Detection**:
- Agent 1: Extract intended behavior from design docs
- Agent 2: Extract actual behavior from implementation
- Agent 3: Extract tested behavior from tests
- Compare: spec says X, code does Y, tests expect Z

**Invariant Extraction**:
- Have agents identify what MUST ALWAYS be true
- Examples: "User balance must never be negative", "Every request must have trace ID"
- Use for test generation and assertion writing

**Competing Swarms** (Adversarial):
- Agent A: Find all problems
- Agent B: Critique Agent A's findings (valid vs false positives?)
- Agent A: Respond to critique
- Converges on agreed-upon findings

#### 4.5.6 SDK Stacking (300 words)
- Use Agent SDK to programmatically launch parallel Claude Code instances
- Architecture: Orchestrator (Agent SDK) → 4-10 Claude Code instances (parallel)
- Each instance runs independently
- Aggregate findings + de-dupe + final report
- Code example: TypeScript using `Promise.all()` with Agent SDK
- When to use:
  - Quick sanity check: 3 agents, 1 run each
  - Pre-deploy: 5 agents, 2 runs = 10 total
  - Security audit: 10 agents, 4 runs = 40 total
  - Trade-off: More cost/latency, but higher confidence

### 4.6 Actor-Critic Adversarial Coding (1,200 words)

**Objectives**: Teach readers to use adversarial self-review to catch issues before human review

#### 4.6.1 The Pattern (300 words)
- Two roles: Actor (code generator) and Critic (code reviewer)
- Iterative refinement loop:
  1. Actor generates code (optimistic, focuses on functionality)
  2. Critic reviews for issues (paranoid, assumes vulnerabilities)
  3. Critic reports findings
  4. Actor refactors to fix issues
  5. Repeat until approved (or max rounds reached)
- Benefits: 3-5 rounds eliminate 90%+ of issues before human review

#### 4.6.2 Implementation Approaches (200 words)

**Approach 1: Single Agent, Dual Roles**
- Same LLM alternates between actor and critic personas
- Simpler to implement (just different prompts)
- Slightly less powerful (same model for both roles)

**Approach 2: Dual Agents**
- Separate agent instances with specialized personas
- More powerful (can use different models/configurations)
- Requires agent orchestration

#### 4.6.3 Eight Critique Dimensions (800 words)

The critic systematically reviews across 8 dimensions:

**1. Security** (10-item checklist)
- Input validation on all user data
- SQL injection prevention (parameterized queries)
- XSS prevention (output escaping)
- CSRF tokens
- Rate limiting
- No hardcoded secrets
- Password hashing
- Token expiration
- Refresh token rotation
- Audit logging

**2. Architecture** (7-item checklist)
- Layer separation
- Services use repositories, not direct DB access
- No business logic in controllers
- Dependencies flow inward
- Domain entities are pure
- Interfaces for external dependencies
- Single Responsibility Principle

**3. Performance** (7-item checklist)
- No N+1 queries
- Database indexes on queried columns
- Caching for frequently accessed data
- Pagination for large result sets
- Async operations for I/O
- Connection pooling
- Lazy loading where appropriate

**4. Testing** (7-item checklist)
- Happy path covered
- Edge cases tested (null, empty, boundaries)
- Error scenarios tested
- Integration tests for critical paths
- Coverage > 80%
- Deterministic (no flaky tests)
- External dependencies mocked

**5. Error Handling** (7-item checklist)
- No swallowed exceptions
- Errors include context
- User-facing errors are actionable
- Internal errors logged with stack traces
- Graceful degradation
- Recovery mechanisms where possible
- No throws in Result-returning functions

**6. Documentation** (6-item checklist)
- JSDoc/TSDoc on public functions
- Inline comments for complex logic
- README updated
- API documentation updated
- Migration guide for breaking changes
- Examples provided

**7. Accessibility** (7-item checklist, if applicable)
- Semantic HTML
- ARIA labels
- Keyboard navigation
- Screen reader friendly
- Color contrast (WCAG AA)
- Visible focus indicators
- Form validation errors announced

**8. Code Quality** (7-item checklist)
- No duplication (DRY)
- Functions < 50 lines
- Descriptive variable names
- No magic numbers
- Consistent naming conventions
- No commented-out code
- Organized imports

#### 4.6.4 Real-World Example: JWT Authentication (500 words)

Walk through 6 rounds of actor-critic on authentication:

- **Round 1: Actor generates**
  - First-pass implementation: SQL injection vulnerability, hardcoded secret, plain text password comparison, no token expiration, no rate limiting
- **Round 2: Critic finds 7 issues**
  - 4 critical (SQL injection, plain passwords, hardcoded secret, no expiration)
  - 3 warnings (no rate limiting, throws exception, no audit logging)
- **Round 3: Actor refactors**
  - Uses parameterized queries, bcrypt, env variables, token expiration, error handling
- **Round 4: Critic re-reviews**
  - Finds 3 remaining issues (rate limiting per IP only, no refresh token, generic error messages)
- **Round 5: Actor refactors again**
  - Adds IP+email rate limiting, refresh token mechanism
- **Round 6: Critic approves**
  - "APPROVED. All security issues resolved. Production-ready."

#### 4.6.5 Cost-Benefit Analysis (400 words)
- Benefits:
  - Higher quality before human review (fewer revision cycles)
  - Security by default (90%+ of OWASP Top 10 caught)
  - Architectural compliance (enforces patterns from CLAUDE.md)
  - Learning tool (critique teaches better patterns)
  - Cost-effective ($0.20-1.00 per feature vs. $50-100 human hour)
- Challenges:
  - Multiple LLM calls add cost (~5 calls per feature)
  - Slower than single-pass (30-60 sec → 2-5 minutes)
  - Risk of infinite loops (need max rounds and convergence detection)
  - Over-engineering (critic might add unnecessary complexity)
- Mitigation strategies:
  - Skip critique for simple changes (docs, trivial fixes)
  - Use cheaper models for critic
  - Limit max rounds (3-5)
  - Fast-path for low-risk changes
  - Detect diminishing returns (stop if improvement < 10%)

### 4.7 Parallel Agents for Monorepo-Wide Changes (1,000 words)

**Objectives**: Teach readers to scale agent work across multiple packages simultaneously

#### 4.7.1 The Sequential Bottleneck (300 words)
- Problem: Updating 20 packages sequentially takes 2.5+ hours
  - Package 1: 8 minutes
  - Package 2: 7 minutes
  - ... (repeat for all 20)
  - Total: 20 × 7.5 min = 150 minutes
- Why sequential fails:
  - Single agent context overloaded (80K tokens of all packages)
  - Inconsistent application across packages
  - Context drift (forgets early decisions)
  - Error accumulation (early errors cascade to dependent packages)
- Real-world example: tRPC v10.0 → v10.45 update
  - 2 hours 15 minutes sequential
  - 4 packages with incorrect imports
  - 2 packages missed deprecated syntax
  - Developer frustration: high

#### 4.7.2 The Parallel Solution (400 words)
- Spawn one agent per package
- Each agent gets focused context:
  - Only one package's files
  - 5K tokens instead of 80K tokens
  - 100% clarity (no confusion)
- Result: 20 packages in parallel = max(individual times) = ~9-15 minutes
- Speedup: 160 / 12 = 13x practical speedup
- Benefits:
  - 10x faster (160 min → 15 min)
  - Consistent (same instructions = same result)
  - Error isolation (one failure doesn't block others)
  - Better context per agent
  - Excellent developer experience

#### 4.7.3 When to Use Parallel Agents (400 words)

**Good use cases**:
- Identical task across packages (update dependency, refactor API calls, add linting rule)
- Independent packages (microservices, component libraries, utilities)
- Clear success criteria (tests pass, linter passes, TypeScript compiles)

**Not ideal**:
- Tasks requiring coordination between packages
- Vague exploratory refactoring
- Tightly coupled packages with shared state

#### 4.7.4 Implementation Steps (400 words)

**Step 1**: Identify packages
- Use monorepo tools (pnpm, yarn, npm, lerna) to list packages

**Step 2**: Define task
- Write clear, focused task description
- Include all changes required
- Specify success criteria

**Step 3**: Spawn agents
- Manual: Create multiple agent tasks in Claude Code
- Automated: Use Agent SDK with `Promise.all()`

**Step 4**: Verify results
- Run `git status` to check modifications
- Run tests across all packages
- Search for old API usage

**Step 5**: Review and commit
- Check diffs for consistency
- Create atomic commit

#### 4.7.5 Advanced Patterns (300 words)

**Pattern 1: Staged Rollout**
- Batch 1: Low-risk packages (tools, utilities)
- Batch 2: Internal services
- Batch 3: Critical services (API gateway)

**Pattern 2: Dependency-Aware Ordering**
- Topological sort of packages
- Update in dependency order (no deps first)

**Pattern 3: Dry Run Mode**
- Test on 2-3 packages first
- If successful, scale to all packages

**Pattern 4: Rollback on Failure**
- Save current git state
- Update in temporary branch
- Rollback if any agent fails
- Merge if all pass

#### 4.7.6 Metrics and Measurement (200 words)
- Time savings: 160 minutes → 15 minutes (10x speedup)
- Error rate: 20% → 0% (consistency improvement)
- Consistency: Check all packages applied changes identically
- Developer satisfaction: Improves from "tedious" to "fire and forget"

### 4.8 Orchestration Patterns (800 words)

**Objectives**: Teach readers to coordinate multiple agents and types of work

#### 4.8.1 Sequential vs Parallel Orchestration (300 words)
- Sequential (feature dependencies):
  - Backend → Frontend → QA → Review
  - Use when agents depend on previous outputs
  - Total time: sum of all times + coordination overhead
- Parallel (independent work):
  - Backend + Frontend simultaneously (independent)
  - Use when agents don't depend on each other
  - Total time: max(concurrent) + coordination overhead
- Mixed:
  - Parallel Backend and Frontend
  - Sequential: (Backend + Frontend) → QA → Review
  - Optimization: Start QA on backend while frontend completes

#### 4.8.2 Handoff Management (300 words)
- Clear handoff points between agents:
  - Backend hands off to Frontend with: endpoint URL, schema, examples
  - Frontend hands off to QA with: component names, prop types, examples
  - QA hands off to Review with: test code, coverage report, edge cases
  - Review reports back to whoever needs to fix issues
- Implementation:
  - Structured output format (JSON or markdown)
  - Explicit dependencies documented
  - Failure modes handled (what if Backend fails?)
  - Rollback procedures (revert and retry with different approach)

#### 4.8.3 Error Handling in Orchestration (200 words)
- Fail fast vs fail forward
  - Fast: If Backend fails, don't continue to Frontend
  - Forward: Mark Backend as blocked, continue with what you have
- Escalation: If agent fails max retries, escalate to human
- Partial success: Report what completed, what failed, what's blocked

#### 4.8.4 Metrics and Monitoring (200 words)
- Track per-agent:
  - Time to completion
  - Quality score (issues found/code quality)
  - Success rate (pass/fail)
- Track system-wide:
  - Total time (start to final result)
  - Quality improvement (vs single agent)
  - Human review cycles saved
  - Cost per feature

### 4.9 Best Practices and Anti-Patterns (900 words)

#### 4.9.1 Best Practices (500 words)

**1. Keep Agent Contexts Focused**
- Each agent should know one domain deeply, not many domains shallowly
- Backend: API patterns, DB schemas, business logic
- Frontend: Component patterns, design system, routing
- ❌ Don't mix domains (don't teach backend about React patterns)

**2. Define Clear Handoff Points**
- Each agent knows when to delegate
- Backend: "Code complete, ready for frontend"
- Frontend: "Component ready, ready for testing"
- QA: "Tests written, ready for review"
- Prevents agents from straying into other domains

**3. Use Read-Only Reviewers**
- Code reviewer can identify but not fix issues
- Prevents reviewers from introducing new bugs
- Forces fixes through proper channels

**4. Monitor Quality Metrics**
- Track: issue density, human review cycles, time-to-production
- Measure improvement vs single agent baseline
- Use metrics to decide: should we add more specialists or consolidate?

**5. Accept the Latency Tradeoff**
- Initial latency is higher (multiple agents + coordination)
- But revision cycles are fewer (fewer human reviews)
- Net result: faster to production despite slower initial generation

#### 4.9.2 Anti-Patterns to Avoid (400 words)

**❌ Anti-Pattern 1: Sub-agents on trivial tasks**
- "Fix this typo" doesn't need a full sub-agent team
- Use main agent for quick tasks
- Sub-agents justified for: features, refactors, architectures

**❌ Anti-Pattern 2: Agents with overlapping responsibilities**
- Don't have two agents doing similar work
- This wastes tokens and creates conflicts
- Clear roles: Backend, Frontend, QA, Review (not "Full-Stack", "Testing", "Quality Assurance")

**❌ Anti-Pattern 3: No tool restrictions**
- If all agents have all tools, they don't specialize
- Restrict tools to role needs (Reviewer has no Edit)
- This forces proper handoffs

**❌ Anti-Pattern 4: Ignoring context layers**
- If agents don't read Root CLAUDE.md, they won't follow patterns
- If agents don't read package-specific CLAUDE.md, they won't match style
- Three layers ensure consistency and specialization

**❌ Anti-Pattern 5: No success criteria**
- If agents don't know what "done" looks like, they won't stop
- Define: "All tests pass", "Linter green", "Coverage > 80%"
- Without criteria, agents can work forever (or produce incomplete work)

---

## 5. Key Examples and Code Snippets

### 5.1 Example 1: Payment Feature (Complete Workflow)
- Sub-agent team structure diagram
- Backend Engineer: Payment schema + service + error handling
- Frontend Engineer: PaymentForm component with Stripe integration
- QA Engineer: Integration test with happy path + edge cases + error scenarios
- Code Reviewer: Security review checklist + findings + approval
- Orchestrator: Aggregated summary

### 5.2 Example 2: JWT Authentication (Actor-Critic Loop)
- Round 1: First-pass code with 7 issues
- Round 2: Critic finds security vulnerabilities
- Round 3: Actor refactors with bcrypt, env secrets, parameterized queries
- Round 4: Critic re-reviews, finds 3 remaining issues
- Round 5: Actor adds rate limiting, refresh tokens
- Round 6: Critic approves (0 issues found)

### 5.3 Example 3: Logger Migration (Parallel Agents)
- Task: Update @company/logger from v2 to v3 across 20 packages
- Parallel agent setup: One agent per package
- Sequential approach: 2.5 hours, 4 packages broken, context lost
- Parallel approach: 15 minutes, 0 broken, each package clear context
- Git log showing atomic commit with all updates

### 5.4 Example 4: Security Swarm (Thoroughness)
- Task: Analyze payment module for vulnerabilities
- Agent 1: Security specialist (OWASP Top 10)
- Agent 2: Performance specialist (N+1 queries, caching)
- Agent 3: Types specialist (TypeScript strict mode)
- Agent 4: Tests specialist (coverage, edge cases)
- Aggregate: Findings ranked by severity and confidence
- Result: 15 issues found (all agents agree on top 5, distributed findings on others)

### 5.5 Example 5: Three-Layer Context Hierarchy
- Root CLAUDE.md: Architecture, TypeScript standards, error handling patterns
- .claude/agents/backend-engineer.md: When implementing endpoint, follow this workflow
- packages/api/CLAUDE.md: tRPC route structure, Zod schemas, Express patterns

---

## 6. Diagrams Needed

**Diagram 1: Team Structure and Handoffs**
- Description: Orchestrator at center, four specialists branching out with handoff arrows and information flow. Shows: task → backend (API) → frontend (component) → QA (tests) → reviewer (audit) → back to orchestrator (aggregation)
- High-level flow: Task → [Backend + Frontend parallel] → QA → Reviewer → Approved

**Diagram 2: Three-Layer Context Hierarchy**
- Description: Pyramid showing Root CLAUDE.md (base, shared patterns), Agent Behavioral Flows (middle, role-specific workflows), Package-Specific Context (top, local conventions). Each layer shows example content and inheritance.
- Shows vertical hierarchy with examples at each level

**Diagram 3: Accuracy vs Latency Trade-off**
- Description: Scatter plot showing:
  - Main agent: Medium accuracy, low latency
  - Sub-agents: High accuracy, high latency
  - Script: Low accuracy (fixed), no latency
  - Shows decision framework: use based on what matters more
- Also shows: token cost, error rate, human review cycles

**Diagram 4: Actor-Critic Loop**
- Description: Circular flow showing:
  - Actor generates code (optimistic)
  - Critic reviews across 8 dimensions (paranoid)
  - Issues found? → Yes: Actor refactors, loop back → No: Approved
  - Shows rounds 1-6 with issue counts decreasing (7 → 4 → 2 → 1 → 0)

**Diagram 5: Swarm Patterns - Many Perspectives**
- Description: Central code/task, surrounded by 10 agent nodes labeled with perspectives (Security, Performance, Types, Tests, Edge Cases, Race Conditions, Memory, API Contracts, Error Handling, Observability). All arrows point to aggregation center. Shows de-duplication and ranking.

**Diagram 6: Sequential vs Parallel Orchestration**
- Description: Two timelines:
  - Sequential: Backend (8m) → Frontend (7m) → QA (6m) → Review (5m) = 26m total
  - Parallel: Backend (8m) + Frontend (7m) [parallel] → QA (6m) → Review (5m) = 19m total
  - Shows parallel packages in monorepo: 20 packages in parallel = 9m instead of 150m

**Diagram 7: Parallel Agents for Monorepo**
- Description: Orchestrator at top, 20 package nodes below, each with parallel agent. Shows: 20 packages × 8m sequential = 160m vs 20 packages parallel = 9m max = 17x speedup
- Each package isolated context (5K tokens) vs shared context (80K tokens)

---

## 7. Exercises: "Try It Yourself"

### 7.1 Exercise 1: Sub-Agent Team Design (Hands-On, 60-90 minutes)

**Task**: Design a sub-agent team for a feature in your own codebase

**Scenario**: You want to add OAuth login to your application

**Instructions**:
1. Identify the specialized agents you'd need (e.g., Backend, Frontend, QA, Review)
2. Write behavioral flows for 2 agents (100-200 lines each)
3. Define tool access control for each agent
4. Create a root CLAUDE.md with shared patterns (200-300 lines)
5. Create package-specific CLAUDE.md for one package (100-150 lines)
6. Design the orchestration flow (sequential vs parallel)

**Deliverables**:
- `.claude/agents/backend-engineer.md` (workflow when implementing OAuth endpoint)
- `.claude/agents/frontend-engineer.md` (workflow when building login form)
- `CLAUDE.md` (root patterns: error handling, naming, architecture)
- `packages/api/CLAUDE.md` (package patterns: route structure, auth approach)
- `orchestration-plan.md` (sequencing and handoff points)

**Evaluation**:
- Are agent roles clearly separated?
- Does each agent know when to delegate?
- Do behavioral flows match your codebase's patterns?
- Are tool restrictions enforced?

### 7.2 Exercise 2: Actor-Critic Loop on Your Code (Hands-On, 45-60 minutes)

**Task**: Run an actor-critic loop on a feature you want to build

**Scenario**: Implement a database schema migration

**Instructions**:
1. Write a critic prompt with all 8 critique dimensions (400-500 lines)
2. Have Claude Code play the actor: generate migration code
3. Have Claude Code play the critic: review the code
4. Count issues found in round 1
5. Have Claude Code play the actor: refactor based on critique
6. Have Claude Code play the critic: re-review
7. Repeat until approved or max 5 rounds

**Deliverables**:
- `migration-actor-prompt.md` (instructions for code generation)
- `migration-critic-prompt.md` (8-dimension review checklist)
- `actor-critic-log.md` (record of all rounds: code + critique + improvements)
- `metrics.md` (issues per round, time to approval, human review time saved)

**Evaluation**:
- Did issue count decrease each round?
- Did all 8 dimensions get checked?
- Would this code pass human review on first try?
- How many human review cycles were eliminated?

### 7.3 Exercise 3: Parallel Agents for Monorepo Update (Hands-On, 30-45 minutes)

**Task**: Run parallel agents to update a dependency across multiple packages

**Scenario**: Update `axios` from v1.0 to v1.4 across 5 packages (or more if available)

**Instructions**:
1. Identify all packages in your monorepo (use pnpm/yarn/npm/lerna tools)
2. Write a clear task description for each agent
3. Spawn 3-5 agents in parallel (in Claude Code chat)
4. Monitor progress
5. Verify changes across all packages (git diff, tests, linter)
6. Measure time saved vs sequential approach

**Deliverables**:
- `parallel-task.md` (clear task description for agents)
- `parallel-agent-results.md` (status per package, time taken, errors)
- `metrics.md` (time: sequential vs parallel, consistency check, error rate)
- `git-commit.md` (atomic commit message showing all changes)

**Evaluation**:
- Did all agents run in parallel?
- Did all packages update correctly?
- Was there consistency across all packages?
- How much time was saved?

---

## 8. Cross-References to Other Chapters

- **Chapter 2: Layered Context Patterns** – Foundation for three-layer hierarchy in sub-agents
- **Chapter 3: Type-Driven Development** – Types provide contracts between sub-agents
- **Chapter 4: Boundary Enforcement** – Tool access control prevents invalid states
- **Chapter 5: Making Invalid States Impossible** – Role-specific invariants constrain sub-agents
- **Chapter 6: Plan Mode for Complex Tasks** – How to brief orchestrator agents on multi-stage tasks
- **Chapter 7: Model Switching Strategy** – Use cheaper models (Haiku) for simple tasks, Sonnet for implementation, Opus for architecture
- **Chapter 8: Trust-But-Verify Protocol** – Verify sub-agent output before accepting
- **Chapter 10: Quality Gates as Information Filters** – Each gate (TypeScript, Linter, Tests, Review) reduces entropy
- **Chapter 11: 24/7 Development Strategy** – Sub-agents enable autonomous night shifts with dedicated QA and review agents
- **Chapter 12: Building the Factory** – Sub-agent architecture as meta-infrastructure; specialized agents are tools that generate code

---

## 9. Word Count Target

**Total chapter target: 8,000-10,000 words**

- Overview: 300 words
- Generalist Trap: 800 words
- Sub-Agent Architecture Solution: 1,200 words
- Real-World Example (Payment): 1,500 words
- Accuracy vs Latency: 900 words
- Agent Swarms: 1,000 words
- Actor-Critic: 1,200 words
- Parallel Agents: 1,000 words
- Orchestration Patterns: 800 words
- Best Practices / Anti-Patterns: 900 words
- **Buffer for refinement**: 400 words

**Allocation for code examples and diagrams**: ~2,000-2,500 words (code snippets, inline examples, diagram descriptions)

**Estimated final word count: 9,000-10,500 words**

---

## 10. Detailed Outline Summary (Reference)

```
Ch 9: Sub-Agent Architecture
├── 4.1 The Generalist Trap (800 words)
│   ├── Scenario: Authentication feature
│   ├── Quality degradation per domain
│   ├── Real-world impact metrics
│   └── Root causes of failure
│
├── 4.2 The Sub-Agent Architecture Solution (1,200 words)
│   ├── 4.2.1 Team Structure (Orchestrator + 4 roles)
│   ├── 4.2.2 Three-Layer Context Hierarchy
│   │   ├── Layer 1: Root CLAUDE.md
│   │   ├── Layer 2: Agent Behavioral Flows
│   │   └── Layer 3: Package-Specific Context
│   ├── 4.2.3 Tool Access Control (Role-Based Permissions)
│   └── 4.2.4 Sequential vs Parallel Execution
│
├── 4.3 Real-World Example: Payment Feature (1,500 words)
│   ├── Step 1: Backend Engineer (API implementation)
│   ├── Step 2: Frontend Engineer (UI component)
│   ├── Step 3: QA Engineer (test suite)
│   ├── Step 4: Code Reviewer (audit + security)
│   └── Step 5: Orchestrator (aggregation)
│
├── 4.4 Accuracy vs Latency Trade-Offs (900 words)
│   ├── Why Sub-Agents Are More Accurate
│   ├── When Sub-Agents Win
│   ├── When Main Agent Wins
│   ├── Decision Framework
│   └── Cost Analysis (Latency vs Revision Cycles)
│
├── 4.5 Agent Swarm Patterns (1,000 words)
│   ├── Core Idea (Multiple Perspectives)
│   ├── Pattern 1: Many Perspectives (10 angles)
│   ├── Pattern 2: Same Perspective Multiple Times (4x for confidence)
│   ├── Pattern 3: Many-Many (10 × 4 = 40 analyses)
│   ├── Specialized Applications (Spec Drift, Invariants, Competing)
│   └── SDK Stacking (Parallel orchestration via Agent SDK)
│
├── 4.6 Actor-Critic Adversarial Coding (1,200 words)
│   ├── The Pattern (Loop: Generate → Critique → Refactor → Repeat)
│   ├── Implementation Approaches (Single Agent vs Dual Agents)
│   ├── Eight Critique Dimensions
│   │   ├── Security (10-item checklist)
│   │   ├── Architecture (7-item checklist)
│   │   ├── Performance (7-item checklist)
│   │   ├── Testing (7-item checklist)
│   │   ├── Error Handling (7-item checklist)
│   │   ├── Documentation (6-item checklist)
│   │   ├── Accessibility (7-item checklist)
│   │   └── Code Quality (7-item checklist)
│   ├── Real-World Example: JWT Auth (6 rounds)
│   └── Cost-Benefit Analysis + Mitigation Strategies
│
├── 4.7 Parallel Agents for Monorepos (1,000 words)
│   ├── The Sequential Bottleneck (20 packages × 8m = 150m)
│   ├── The Parallel Solution (20 packages in parallel = 12m max)
│   ├── When to Use Parallel Agents
│   ├── Implementation Steps (5 steps)
│   ├── Advanced Patterns (Staged, Dependency-Aware, Dry Run, Rollback)
│   └── Metrics and Measurement
│
├── 4.8 Orchestration Patterns (800 words)
│   ├── Sequential vs Parallel Orchestration
│   ├── Handoff Management (Clear Transition Points)
│   ├── Error Handling (Fail Fast vs Forward, Escalation)
│   └── Metrics and Monitoring
│
└── 4.9 Best Practices & Anti-Patterns (900 words)
    ├── Best Practices (5: Focused Context, Clear Handoffs, Read-Only Reviewers, Metrics, Accept Latency)
    └── Anti-Patterns (5: Trivial Tasks, Overlapping Roles, No Tool Restrictions, No Context Layers, No Success Criteria)
```

---

## 11. Status and Next Steps

**Status**: DRAFT

**Ready for**:
- Content author (James Phoenix) to refine outline
- Example code to be written in full
- Diagrams to be created (mermaid or similar)
- Exercises to be tested with real code

**Not yet completed**:
- Full prose chapter (currently is structured outline)
- All code examples in full
- Diagram creation
- Exercise validation
- Integration with full book structure
- Reader testing / feedback cycles

**Estimated timeline to completion**: 3-4 weeks with full-time writing + testing

---

## Appendix: Key Terminology

| Term | Definition |
|------|-----------|
| **Sub-Agent** | Specialized AI assistant with focused expertise, limited tools, and specific behavioral flow |
| **Orchestrator** | Main agent that coordinates work distribution and handoffs between sub-agents |
| **Three-Layer Hierarchy** | Context structure: Root CLAUDE.md (shared) → Agent behavioral flows (role-specific) → Package-specific context (local) |
| **Behavioral Flow** | Step-by-step workflow that guides an agent through a task (e.g., "When implementing endpoint: 1. understand requirements 2. design endpoint 3. implement layers...") |
| **Tool Access Control** | Restricting which tools each agent can use (e.g., reviewer has only Read/Grep/Glob, no Edit) |
| **Handoff** | Transition point where one agent completes work and passes to next agent with structured output |
| **Actor-Critic** | Iterative refinement pattern where actor generates code and critic reviews it across multiple dimensions |
| **Critique Dimension** | One aspect of code quality (security, performance, testing, error handling, architecture, documentation, accessibility, code quality) |
| **Swarm** | Multiple agents running in parallel on same problem from different perspectives or with multiple runs |
| **De-duplication** | Removing duplicate findings from swarm, ranking by confidence (found by multiple agents > found once) |
| **Parallel Agents** | Multiple agents working simultaneously on independent packages (monorepo pattern) |
| **Orchestration** | Coordinating multiple agents with proper sequencing, handoffs, and error handling |

---

## Appendix: Related Concepts and References

### Internal Knowledge Base
- [[sub-agent-architecture|Sub-Agent Architecture]] – Full implementation guide
- [[sub-agents-accuracy-vs-latency|Sub-agents: Accuracy vs Latency]] – Trade-off analysis
- [[agent-swarm-patterns-for-thoroughness|Agent Swarm Patterns]] – Thorough analysis through multiple agents
- [[actor-critic-adversarial-coding|Actor-Critic Adversarial Coding]] – Self-review refinement loops
- [[parallel-agents-for-monorepos|Parallel Agents for Monorepos]] – Scaling across packages

### External References
- [Claude Code Documentation](https://docs.anthropic.com/claude/docs/claude-code)
- [Multi-Agent Systems Research](https://arxiv.org/abs/2308.00352)
- [OWASP Top 10](https://owasp.org/www-project-top-ten/) (security dimension reference)

---

**End of PRD**

*This document serves as the blueprint for Chapter 9 authoring. Content author should use this outline to structure writing, validate examples, and ensure all learning objectives are met.*
